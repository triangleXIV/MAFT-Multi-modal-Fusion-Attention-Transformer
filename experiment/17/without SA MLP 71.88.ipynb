{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a53601-3667-489d-8763-ef88e06264c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: yacs in /usr/local/miniconda3/lib/python3.8/site-packages (0.1.8)\n",
      "Requirement already satisfied: timm in /usr/local/miniconda3/lib/python3.8/site-packages (0.9.10)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/miniconda3/lib/python3.8/site-packages (0.1.99)\n",
      "Requirement already satisfied: regex in /usr/local/miniconda3/lib/python3.8/site-packages (2023.10.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/miniconda3/lib/python3.8/site-packages (from yacs) (6.0)\n",
      "Requirement already satisfied: torch>=1.7 in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (2.0.0+cu117)\n",
      "Requirement already satisfied: torchvision in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.15.1+cu117)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.19.4)\n",
      "Requirement already satisfied: safetensors in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.4.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (2.0.0)\n",
      "Requirement already satisfied: sympy in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.9.0)\n",
      "Requirement already satisfied: cmake in /usr/local/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (15.0.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2023.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (23.0)\n",
      "Requirement already satisfied: requests in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2.29.0)\n",
      "Requirement already satisfied: numpy in /usr/local/miniconda3/lib/python3.8/site-packages (from torchvision->timm) (1.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from torchvision->timm) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/miniconda3/lib/python3.8/site-packages (from sympy->torch>=1.7->timm) (1.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yacs timm sentencepiece regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889c919a-b4e7-40c7-acc4-e14f59053eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from yacs.config import CfgNode as CN\n",
    "from tqdm import tqdm,trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from toolcls import AbmsaProcessor,seed_everything,convert_mm_examples_to_features,BertConfig\n",
    "from models.swin.swintransformer import SwinTransformer,get_config\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from models.deberta.spm_tokenizer import SPMTokenizer\n",
    "from models.deberta.deberta import SwinBERTa\n",
    "from models.logs import logger\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import safetensors.torch\n",
    "    _has_safetensors = True\n",
    "except ImportError:\n",
    "    _has_safetensors = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b10da-e9f2-4d6c-bffc-ae6e5b6fd01f",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cec1ab2-a125-4daa-8ee0-bb0c24fca243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img_encoder config\n",
    "_C = CN()\n",
    "config = _C.clone()\n",
    "config.LOCAL_RANK = -1\n",
    "\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return (1-x)/(1-warmup)\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    preds = np.argmax(y_pred, axis=-1)\n",
    "    true = y_true\n",
    "    p_macro, r_macro, f_macro, support_macro \\\n",
    "      = precision_recall_fscore_support(true, preds, average='macro')\n",
    "    #f_macro = 2*p_macro*r_macro/(p_macro+r_macro)\n",
    "    return p_macro, r_macro, f_macro\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "## Required parameters\n",
    "parser.add_argument(\"--data_dir\",default='./twitterdataset/absa_data/twitter',type=str,#文本数据位置\n",
    "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "parser.add_argument(\"--img_ckpt\", default='./pretrains/swin_base_patch4_window7_224_1k.pth', type=str,#swin预训练模型的准确位置\n",
    "                    help=\"Bert pre-trained model selected in the list: S24-224,S24-336 \")\n",
    "parser.add_argument(\"--spm_model_file\",default='./pretrains/30k-clean.model',type=str)#albert预训练模型的准确位置\n",
    "parser.add_argument(\"--model_name_or_path\", default='./pretrains', type=str,#albert预训练模型存放的文件夹\n",
    "                    help=\"Path to pre-trained model or shortcut name selected in the list\")\n",
    "parser.add_argument(\"--task_name\",default='twitter17',type=str,#要加载哪个数据集 Twitter是17 Twitter15是15\n",
    "                    help=\"The name of the task to train.\")\n",
    "parser.add_argument(\"--output_dir\",default='./output',type=str,\n",
    "                    help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "parser.add_argument('--path_image', default='./twitterdataset/img_data/twitter2017_images',#图像的位置\n",
    "                    help='path to images')\n",
    "parser.add_argument('--init_model',\n",
    "                    type=str,\n",
    "                    default='./pretrains/pytorch_model.bin',\n",
    "                    help=\"The model state file used to initialize the model weights.\")\n",
    "parser.add_argument('--model_config',\n",
    "                    type=str,\n",
    "                    default='./pretrains/config.json',\n",
    "                    help=\"The config file of bert model.\")\n",
    "parser.add_argument('--pre_trained',\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    help=\"The path of pre-trained RoBERTa model\")\n",
    "parser.add_argument('--vocab_path',\n",
    "                    default='./pretrains/spm.model',\n",
    "                    type=str,\n",
    "                    help=\"The path of the vocabulary\")\n",
    "## Other parameters\n",
    "parser.add_argument('--crop_size', type=int, default=224, help='crop size of image')\n",
    "parser.add_argument(\"--max_seq_length\",default=64,type=int,\n",
    "                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--max_entity_length\",default=16,type=int,\n",
    "                    help=\"The maximum entity input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--do_train\",action='store_true',default=True,\n",
    "                    help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_lower_case\",action='store_true',default=True,\n",
    "                    help=\"Set this flag if you are using an uncased model.\")\n",
    "parser.add_argument(\"--train_batch_size\",default=24,type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\",default=24,type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--learning_rate\",default=3e-5,type=float,#可能会暴毙\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\",default=8.0,type=float,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\",default=0.1,type=float,\n",
    "                    help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                         \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",action='store_true',default=False,\n",
    "                    help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",type=int,default=-1,\n",
    "                    help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed',type=int,default=2,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument('--gradient_accumulation_steps',type=int,default=1,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument('--fp16',default=False,action='store_true',#kaggle没搞懂怎么导入apex 索性就32位吧\n",
    "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--loss_scale',type=float, default=0,\n",
    "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                         \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                         \"Positive power of 2: static loss scaling value.\\n\")\n",
    "parser.add_argument('--overwrite_output_dir', action='store_true',default=True,#是否覆盖原本的输出文件\n",
    "                    help=\"Overwrite the content of the output directory\")\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument('--cfg', type=str, default=\"./pretrains/swin_base_patch4_window7_224.yaml\", metavar=\"FILE\",\n",
    "                    help='path to config file', )\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e4b1e-0876-45c8-ba8d-f362f357e2f8",
   "metadata": {},
   "source": [
    "# 基础随机数 cuda 路径之类的东西配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b7ddc1-de3f-448d-8a66-3e80053920a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.task_name == \"twitter17\":\n",
    "    args.path_image = \"./twitterdataset/img_data/twitter2017_images\"\n",
    "elif args.task_name == \"twitter15\":\n",
    "    args.path_image = \"./twitterdataset/img_data/twitter2015_images\"\n",
    "else:\n",
    "    print(\"The task name is not right!\")\n",
    "processors = {\n",
    "        \"twitter15\": AbmsaProcessor,    # our twitter-2015 dataset\n",
    "        \"twitter17\": AbmsaProcessor         # our twitter-2017 dataset\n",
    "}\n",
    "num_labels_task = {\n",
    "    \"twitter15\": 3,                # our twitter-2015 dataset\n",
    "    \"twitter17\": 3                     # our twitter-2017 dataset\n",
    "}\n",
    "seed_everything(args.seed) #固定随机数种子\n",
    "task_name = args.task_name.lower()\n",
    "#初始化输出的文件夹\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "args.output_dir = args.output_dir\n",
    "if os.path.exists(args.output_dir) and os.listdir(\n",
    "        args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir))\n",
    "#设置cuda\n",
    "if config.LOCAL_RANK == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(config.LOCAL_RANK)\n",
    "    device = torch.device(\"cuda\", config.LOCAL_RANK)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "processor = processors[task_name]()#获得读取tsv文件方法\n",
    "num_labels = num_labels_task[task_name]#判定几分类\n",
    "label_list = processor.get_labels()#获得分类标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bb07a-89a6-4764-831f-b0fd96519557",
   "metadata": {},
   "source": [
    "# 定义模型 载入预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d989f45-96e0-49fe-b181-039aeca4cb67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:27:04,344 - root - INFO - Loaded pretrained model file ./pretrains/pytorch_model.bin\n",
      "2023-11-20 14:27:05,076 - root - WARNING - Missing keys: [], unexpected_keys: [], error_msgs: []\n",
      "2023-11-20 14:27:05,178 - root - INFO - Loaded pretrained model file ./pretrains/pytorch_model.bin\n",
      "2023-11-20 14:27:05,917 - root - WARNING - Missing keys: [], unexpected_keys: [], error_msgs: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from ./pretrains/swin_base_patch4_window7_224.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.022)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.043)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.065)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.109)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.130)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.152)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.174)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.196)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.217)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.239)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.261)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.283)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.304)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.326)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.348)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.370)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.391)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.413)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.435)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.457)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.478)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.500)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = os.path.join(args.model_name_or_path, 'bert_config.json')\n",
    "bert_config = BertConfig.from_json_file(config_file)\n",
    "# 读取词表 方便之后把文字转数字id 返回的是一个30000词字典\n",
    "tokenizer = SPMTokenizer(args.vocab_path)\n",
    "#创建并导入预训练模型 这个模型用于文本encoder 融合图文feature\n",
    "model=SwinBERTa(args, bert_config,args.init_model)\n",
    "model.to(device)\n",
    "#设定图像encoder \n",
    "config = get_config(args)\n",
    "encoder = SwinTransformer(img_size=224,\n",
    "                          patch_size=config.MODEL.SWIN.PATCH_SIZE,\n",
    "                          in_chans=config.MODEL.SWIN.IN_CHANS,\n",
    "                          num_classes=config.MODEL.NUM_CLASSES,\n",
    "                          embed_dim=config.MODEL.SWIN.EMBED_DIM,\n",
    "                          depths=config.MODEL.SWIN.DEPTHS,\n",
    "                          num_heads=config.MODEL.SWIN.NUM_HEADS,\n",
    "                          window_size=config.MODEL.SWIN.WINDOW_SIZE,\n",
    "                          mlp_ratio=config.MODEL.SWIN.MLP_RATIO,\n",
    "                          qkv_bias=config.MODEL.SWIN.QKV_BIAS,\n",
    "                          qk_scale=config.MODEL.SWIN.QK_SCALE,\n",
    "                          drop_rate=config.MODEL.DROP_RATE,\n",
    "                          drop_path_rate=config.MODEL.DROP_PATH_RATE,\n",
    "                          patch_norm=config.MODEL.SWIN.PATCH_NORM,\n",
    "                          use_checkpoint=False)\n",
    "pretrained_dict = torch.load(args.img_ckpt, map_location='cpu')\n",
    "pretrained_dict = pretrained_dict['model']\n",
    "unexpected_keys = {\"head.weight\", \"head.bias\"}\n",
    "# 删除不匹配的键值\n",
    "for key in unexpected_keys:\n",
    "    del pretrained_dict[key]\n",
    "missing_keys, unexpected_keys = encoder.load_state_dict(pretrained_dict, strict=False)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d60ab-7d42-45bd-bf0f-4df840e90163",
   "metadata": {},
   "source": [
    "# 加载优化器 pth保存路径配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdb8aab-870e-4951-914b-77dd9117c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:27:08,323 - root - INFO - LOOKING AT ./twitterdataset/absa_data/twitter/train.tsv\n"
     ]
    }
   ],
   "source": [
    "train_examples = processor.get_train_examples(args.data_dir)#获取训练集文本内容\n",
    "eval_examples = processor.get_dev_examples(args.data_dir)#获取验证集文本内容\n",
    "num_train_steps = int(len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
    "t_total = num_train_steps\n",
    "#文本和融合部分参数的优化策略\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "#图像部分参数优化策略\n",
    "def check_keywords_in_name(name, keywords=()):\n",
    "    isin = False\n",
    "    for keyword in keywords:\n",
    "        if keyword in name:\n",
    "            isin = True\n",
    "    return isin\n",
    "def set_weight_decay(model, skip_list=(), skip_keywords=()):\n",
    "    has_decay = []\n",
    "    no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n",
    "                check_keywords_in_name(name, skip_keywords):\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            has_decay.append(param)\n",
    "    return [{'params': has_decay,'weight_decay': 0.01},\n",
    "            {'params': no_decay, 'weight_decay': 0.}]\n",
    "skip = {'absolute_pos_embed'}\n",
    "skip_keywords = {'relative_position_bias_table'}\n",
    "optimizer_grouped_parameters2 = set_weight_decay(encoder, skip, skip_keywords)\n",
    "#合并两组参数统一传入adamw优化器调参\n",
    "optimizer_grouped_parameters = optimizer_grouped_parameters1 + optimizer_grouped_parameters2\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate,eps=1e-6)\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "t_total = num_train_steps\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=int(t_total * args.warmup_proportion), t_total=t_total)\n",
    "output_model_file = os.path.join(args.output_dir, \"pytorch_model.pth\")\n",
    "output_encoder_file = os.path.join(args.output_dir, \"pytorch_encoder.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0741964-7b04-4bec-a153-b4506af1fde7",
   "metadata": {},
   "source": [
    "# 载入训练集 验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4570d1-b61a-4e0e-8c8b-ac2e3e86103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:27:08,351 - root - INFO - *** Example ***\n",
      "2023-11-20 14:27:08,351 - root - INFO - guid: train-1\n",
      "2023-11-20 14:27:08,351 - root - INFO - tokens: [CLS] ▁how ▁$ t $ ▁is ▁changing ▁the ▁influencer ▁game ▁: [SEP] ▁jake ▁paul [SEP]\n",
      "2023-11-20 14:27:08,352 - root - INFO - input_ids: 1 361 419 297 1814 269 2198 262 29655 522 877 2 109757 38723 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-20 14:27:08,352 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-20 14:27:08,353 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-20 14:27:08,353 - root - INFO - label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 134\n",
      "the max length of sentence a: 49 entity b: 10 total length: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:27:29,119 - root - INFO - *** Example ***\n",
      "2023-11-20 14:27:29,120 - root - INFO - guid: dev-1\n",
      "2023-11-20 14:27:29,120 - root - INFO - tokens: [CLS] ▁looking ▁forward ▁to ▁the ▁$ t $ ▁from ▁4 ▁- ▁8 ▁july ▁! ▁more ▁info ▁here ▁# ▁heritage ▁# ▁music [SEP] ▁f other ing hay ▁festival [SEP]\n",
      "2023-11-20 14:27:29,121 - root - INFO - input_ids: 1 562 939 264 262 419 297 1814 292 453 341 578 52434 1084 310 2470 422 953 5456 953 755 2 2994 10705 510 28577 3694 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-20 14:27:29,121 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-20 14:27:29,121 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-20 14:27:29,122 - root - INFO - label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 29\n",
      "the max length of sentence a: 48 entity b: 10 total length: 55\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_mm_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "            args.path_image)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in train_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in train_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in train_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in train_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                           all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids,\n",
    "                           all_img_feats, all_label_ids)\n",
    "if args.local_rank == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size,\n",
    "                              drop_last=True)\n",
    "# 获取验证集\n",
    "eval_features = convert_mm_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "    args.path_image)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in eval_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                          all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids, \\\n",
    "                          all_img_feats, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size, drop_last=True)\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "max_acc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2576d-968e-4f67-b827-48e0335ebc75",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a3d005a-22b4-48e1-bb7e-70278a137982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:27:36,318 - root - INFO - *************** Running training ***************\n",
      "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]2023-11-20 14:27:36,320 - root - INFO - ********** Epoch: 0 **********\n",
      "2023-11-20 14:27:36,320 - root - INFO -   Num examples = 3562\n",
      "2023-11-20 14:27:36,320 - root - INFO -   Batch size = 24\n",
      "2023-11-20 14:27:36,320 - root - INFO -   Num steps = 1187\n",
      "Iteration:   0%|          | 0/148 [00:00<?, ?it/s]/usr/local/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Iteration (loss: 0.6035,lr:0.0000299417): 100%|██████████| 148/148 [00:34<00:00,  4.25it/s]\n",
      "2023-11-20 14:28:11,183 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-20 14:28:11,183 - root - INFO -   Num examples = 1176\n",
      "2023-11-20 14:28:11,183 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.4736): 100%|██████████| 49/49 [00:04<00:00, 11.14it/s]\n",
      "2023-11-20 14:28:15,586 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-20 14:28:15,586 - root - INFO -   eval_accuracy = 0.5986394557823129\n",
      "2023-11-20 14:28:15,587 - root - INFO -   eval_loss = 0.8098106432934197\n",
      "2023-11-20 14:28:15,587 - root - INFO -   f_score = 0.5765564123082337\n",
      "2023-11-20 14:28:15,587 - root - INFO -   global_step = 148\n",
      "2023-11-20 14:28:15,587 - root - INFO -   loss = 0.9177108260022627\n",
      "Epoch:  12%|█▎        | 1/8 [00:40<04:45, 40.82s/it]2023-11-20 14:28:17,137 - root - INFO - ********** Epoch: 1 **********\n",
      "2023-11-20 14:28:17,138 - root - INFO -   Num examples = 3562\n",
      "2023-11-20 14:28:17,138 - root - INFO -   Batch size = 24\n",
      "2023-11-20 14:28:17,138 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.5669,lr:0.0000279941): 100%|██████████| 148/148 [00:33<00:00,  4.36it/s]\n",
      "2023-11-20 14:28:51,111 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-20 14:28:51,112 - root - INFO -   Num examples = 1176\n",
      "2023-11-20 14:28:51,112 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.3982): 100%|██████████| 49/49 [00:04<00:00, 11.14it/s]\n",
      "2023-11-20 14:28:55,516 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-20 14:28:55,517 - root - INFO -   eval_accuracy = 0.6113945578231292\n",
      "2023-11-20 14:28:55,517 - root - INFO -   eval_loss = 0.8281762824983013\n",
      "2023-11-20 14:28:55,517 - root - INFO -   f_score = 0.5407644730492486\n",
      "2023-11-20 14:28:55,517 - root - INFO -   global_step = 296\n",
      "2023-11-20 14:28:55,518 - root - INFO -   loss = 0.6694804448130969\n",
      "Epoch:  25%|██▌       | 2/8 [01:20<04:01, 40.29s/it]2023-11-20 14:28:57,060 - root - INFO - ********** Epoch: 2 **********\n",
      "2023-11-20 14:28:57,060 - root - INFO -   Num examples = 3562\n",
      "2023-11-20 14:28:57,061 - root - INFO -   Batch size = 24\n",
      "2023-11-20 14:28:57,061 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.4505,lr:0.0000236267): 100%|██████████| 148/148 [00:33<00:00,  4.37it/s]\n",
      "2023-11-20 14:29:30,932 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-20 14:29:30,932 - root - INFO -   Num examples = 1176\n",
      "2023-11-20 14:29:30,933 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.5277): 100%|██████████| 49/49 [00:04<00:00, 11.12it/s]\n",
      "2023-11-20 14:29:35,348 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-20 14:29:35,348 - root - INFO -   eval_accuracy = 0.6649659863945578\n",
      "2023-11-20 14:29:35,348 - root - INFO -   eval_loss = 0.7799521009532773\n",
      "2023-11-20 14:29:35,349 - root - INFO -   f_score = 0.6218984003679476\n",
      "2023-11-20 14:29:35,349 - root - INFO -   global_step = 444\n",
      "2023-11-20 14:29:35,349 - root - INFO -   loss = 0.4395055311757165\n",
      "Epoch:  38%|███▊      | 3/8 [02:00<03:20, 40.09s/it]2023-11-20 14:29:36,916 - root - INFO - ********** Epoch: 3 **********\n",
      "2023-11-20 14:29:36,916 - root - INFO -   Num examples = 3562\n",
      "2023-11-20 14:29:36,917 - root - INFO -   Batch size = 24\n",
      "2023-11-20 14:29:36,917 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.2183,lr:0.0000176529): 100%|██████████| 148/148 [00:33<00:00,  4.36it/s]\n",
      "2023-11-20 14:30:10,866 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-20 14:30:10,866 - root - INFO -   Num examples = 1176\n",
      "2023-11-20 14:30:10,866 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.4582): 100%|██████████| 49/49 [00:04<00:00, 11.12it/s]\n",
      "2023-11-20 14:30:15,279 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-20 14:30:15,279 - root - INFO -   eval_accuracy = 0.6879251700680272\n",
      "2023-11-20 14:30:15,280 - root - INFO -   eval_loss = 0.9603849214558698\n",
      "2023-11-20 14:30:15,280 - root - INFO -   f_score = 0.6590093590489873\n",
      "2023-11-20 14:30:15,280 - root - INFO -   global_step = 592\n",
      "2023-11-20 14:30:15,280 - root - INFO -   loss = 0.2509911747498287\n",
      "Epoch:  50%|█████     | 4/8 [02:40<02:40, 40.02s/it]2023-11-20 14:30:16,829 - root - INFO - ********** Epoch: 4 **********\n",
      "2023-11-20 14:30:16,830 - root - INFO -   Num examples = 3562\n",
      "2023-11-20 14:30:16,830 - root - INFO -   Batch size = 24\n",
      "2023-11-20 14:30:16,830 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.1752,lr:0.0000111852): 100%|██████████| 148/148 [00:33<00:00,  4.36it/s]\n",
      "2023-11-20 14:30:50,815 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-20 14:30:50,816 - root - INFO -   Num examples = 1176\n",
      "2023-11-20 14:30:50,816 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.6096): 100%|██████████| 49/49 [00:04<00:00, 11.13it/s]\n",
      "2023-11-20 14:30:55,226 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-20 14:30:55,226 - root - INFO -   eval_accuracy = 0.6913265306122449\n",
      "2023-11-20 14:30:55,227 - root - INFO -   eval_loss = 1.0711794778400539\n",
      "2023-11-20 14:30:55,227 - root - INFO -   f_score = 0.690949199301718\n",
      "2023-11-20 14:30:55,227 - root - INFO -   global_step = 740\n",
      "2023-11-20 14:30:55,227 - root - INFO -   loss = 0.11818071293040507\n",
      "Epoch:  62%|██████▎   | 5/8 [03:20<01:59, 39.98s/it]2023-11-20 14:30:56,725 - root - INFO - ********** Epoch: 5 **********\n",
      "2023-11-20 14:30:56,726 - root - INFO -   Num examples = 3562\n",
      "2023-11-20 14:30:56,726 - root - INFO -   Batch size = 24\n",
      "2023-11-20 14:30:56,727 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.0410,lr:0.0000054278): 100%|██████████| 148/148 [00:33<00:00,  4.36it/s]\n",
      "2023-11-20 14:31:30,696 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-20 14:31:30,696 - root - INFO -   Num examples = 1176\n",
      "2023-11-20 14:31:30,697 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.6269): 100%|██████████| 49/49 [00:04<00:00, 11.14it/s]\n",
      "2023-11-20 14:31:35,102 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-20 14:31:35,102 - root - INFO -   eval_accuracy = 0.7006802721088435\n",
      "2023-11-20 14:31:35,103 - root - INFO -   eval_loss = 1.2115885901207826\n",
      "2023-11-20 14:31:35,103 - root - INFO -   f_score = 0.6885801614460019\n",
      "2023-11-20 14:31:35,103 - root - INFO -   global_step = 888\n",
      "2023-11-20 14:31:35,103 - root - INFO -   loss = 0.0490807548284883\n",
      "Epoch:  75%|███████▌  | 6/8 [04:00<01:19, 39.88s/it]2023-11-20 14:31:36,427 - root - INFO - ********** Epoch: 6 **********\n",
      "2023-11-20 14:31:36,428 - root - INFO -   Num examples = 3562\n",
      "2023-11-20 14:31:36,428 - root - INFO -   Batch size = 24\n",
      "2023-11-20 14:31:36,428 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.0152,lr:0.0000014529): 100%|██████████| 148/148 [00:33<00:00,  4.37it/s]\n",
      "2023-11-20 14:32:10,271 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-20 14:32:10,271 - root - INFO -   Num examples = 1176\n",
      "2023-11-20 14:32:10,271 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.6602): 100%|██████████| 49/49 [00:04<00:00, 11.12it/s]\n",
      "2023-11-20 14:32:14,683 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-20 14:32:14,683 - root - INFO -   eval_accuracy = 0.7023809523809523\n",
      "2023-11-20 14:32:14,683 - root - INFO -   eval_loss = 1.2933717100900046\n",
      "2023-11-20 14:32:14,684 - root - INFO -   f_score = 0.6927473629895142\n",
      "2023-11-20 14:32:14,684 - root - INFO -   global_step = 1036\n",
      "2023-11-20 14:32:14,684 - root - INFO -   loss = 0.026248732665113197\n",
      "Epoch:  88%|████████▊ | 7/8 [04:39<00:39, 39.86s/it]2023-11-20 14:32:16,237 - root - INFO - ********** Epoch: 7 **********\n",
      "2023-11-20 14:32:16,237 - root - INFO -   Num examples = 3562\n",
      "2023-11-20 14:32:16,238 - root - INFO -   Batch size = 24\n",
      "2023-11-20 14:32:16,238 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.0126,lr:0.0000000006): 100%|██████████| 148/148 [00:33<00:00,  4.36it/s]\n",
      "2023-11-20 14:32:50,155 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-20 14:32:50,155 - root - INFO -   Num examples = 1176\n",
      "2023-11-20 14:32:50,155 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.6435): 100%|██████████| 49/49 [00:04<00:00, 11.14it/s]\n",
      "2023-11-20 14:32:54,562 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-20 14:32:54,562 - root - INFO -   eval_accuracy = 0.7040816326530612\n",
      "2023-11-20 14:32:54,562 - root - INFO -   eval_loss = 1.2943693274746135\n",
      "2023-11-20 14:32:54,563 - root - INFO -   f_score = 0.694524348309311\n",
      "2023-11-20 14:32:54,563 - root - INFO -   global_step = 1184\n",
      "2023-11-20 14:32:54,563 - root - INFO -   loss = 0.014596204608490036\n",
      "Epoch: 100%|██████████| 8/8 [05:19<00:00, 39.98s/it]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"*************** Running training ***************\")\n",
    "for train_idx in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "    logger.info(\"********** Epoch: \" + str(train_idx) + \" **********\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "    model.train()\n",
    "    encoder.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), desc=\"Iteration\", total=len(train_dataloader), position=0)\n",
    "    for step, batch in progress_bar:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "            img_feats, label_ids = batch\n",
    "        img_att = encoder(img_feats)\n",
    "        loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                         s2_input_mask, \\\n",
    "                         added_input_mask, label_ids)\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        scheduler.step()  # 使用学习率调整算法\n",
    "        for param_group in optimizer.param_groups:\n",
    "            progress_bar.set_description(f\"Iteration (loss: {loss.item():.4f},lr:{param_group['lr']:.10f})\")\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    logger.info(\"***** Running evaluation on Dev Set*****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    model.eval()\n",
    "    encoder.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_label_list = []\n",
    "    pred_label_list = []\n",
    "    #验证\n",
    "    progress_bar = tqdm(eval_dataloader, desc=\"Evaluating\", position=0)\n",
    "    for input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "            img_feats, label_ids in progress_bar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        added_input_mask = added_input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        s2_input_ids = s2_input_ids.to(device)\n",
    "        s2_input_mask = s2_input_mask.to(device)\n",
    "        s2_segment_ids = s2_segment_ids.to(device)\n",
    "        img_feats = img_feats.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            img_att = encoder(img_feats)\n",
    "            tmp_eval_loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids,\n",
    "                                  input_mask, s2_input_mask, added_input_mask, label_ids)\n",
    "            logits = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                           s2_input_mask, added_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        true_label_list.append(label_ids)\n",
    "        pred_label_list.append(logits)\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        progress_bar.set_description(f\"Evaluating (tmp_eval_loss: {tmp_eval_loss.item():.4f})\")\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    loss = tr_loss / nb_tr_steps if args.do_train else None\n",
    "    true_label = np.concatenate(true_label_list)\n",
    "    pred_outputs = np.concatenate(pred_label_list)\n",
    "    precision, recall, F_score = macro_f1(true_label, pred_outputs)\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "              'f_score': F_score,\n",
    "              'global_step': global_step,\n",
    "              'loss': loss}\n",
    "    logger.info(\"***** Dev Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "    if eval_accuracy >= max_acc:\n",
    "        # Save a trained model\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        encoder_to_save = encoder.module if hasattr(encoder,\n",
    "                                                    'module') else encoder  # Only save the model it-self\n",
    "        if args.do_train:\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "            torch.save(encoder_to_save.state_dict(), output_encoder_file)\n",
    "        max_acc = eval_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa03d8-dfde-4274-8d3f-b544be05ea97",
   "metadata": {},
   "source": [
    "# 取最好的结果在测试集上验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec52fd4-39df-45aa-a364-7d2c68529d65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:32:57,168 - root - INFO - *** Example ***\n",
      "2023-11-20 14:32:57,169 - root - INFO - guid: test-1\n",
      "2023-11-20 14:32:57,169 - root - INFO - tokens: [CLS] ▁# ▁$ t $ ▁performs ▁at ▁stagecoach ▁# ▁music festival ▁2016 [SEP] ▁sam hunt [SEP]\n",
      "2023-11-20 14:32:57,169 - root - INFO - input_ids: 1 953 419 297 1814 8993 288 109755 953 755 47550 892 2 20782 39396 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-20 14:32:57,170 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-20 14:32:57,170 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-20 14:32:57,170 - root - INFO - label: 2 (id = 2)\n",
      "2023-11-20 14:33:04,959 - root - INFO - ***** Running evaluation on Test Set*****\n",
      "2023-11-20 14:33:04,959 - root - INFO -   Num examples = 1234\n",
      "2023-11-20 14:33:04,960 - root - INFO -   Batch size = 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 62\n",
      "the max length of sentence a: 71 entity b: 10 total length: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 52/52 [00:04<00:00, 11.43it/s]\n",
      "2023-11-20 14:33:09,666 - root - INFO - ***** Test Eval results *****\n",
      "2023-11-20 14:33:09,666 - root - INFO -   eval_accuracy = 0.7188006482982172\n",
      "2023-11-20 14:33:09,667 - root - INFO -   eval_loss = 1.222228281199932\n",
      "2023-11-20 14:33:09,667 - root - INFO -   f_score = 0.7084232209774263\n",
      "2023-11-20 14:33:09,667 - root - INFO -   global_step = 1184\n",
      "2023-11-20 14:33:09,668 - root - INFO -   loss = 0.014596204608490036\n",
      "2023-11-20 14:33:09,668 - root - INFO -   precision = 0.7225493739749931\n",
      "2023-11-20 14:33:09,668 - root - INFO -   recall = 0.6974078130981586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.222228281199932, 'eval_accuracy': 0.7188006482982172, 'precision': 0.7225493739749931, 'recall': 0.6974078130981586, 'f_score': 0.7084232209774263, 'global_step': 1184, 'loss': 0.014596204608490036}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # 先把cuda清空了\n",
    "model.load_state_dict(torch.load(output_model_file))# 载入eval最好的结果\n",
    "encoder.load_state_dict(torch.load(output_encoder_file))# 载入eval最好的结果\n",
    "eval_examples = processor.get_test_examples(args.data_dir)# 获得测试集\n",
    "eval_features = convert_mm_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "    args.path_image)\n",
    "logger.info(\"***** Running evaluation on Test Set*****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in eval_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                          all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids,\n",
    "                          all_img_feats, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "model.eval()\n",
    "encoder.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "true_label_list = []\n",
    "pred_label_list = []\n",
    "for input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "        img_feats, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    added_input_mask = added_input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    s2_input_ids = s2_input_ids.to(device)\n",
    "    s2_input_mask = s2_input_mask.to(device)\n",
    "    s2_segment_ids = s2_segment_ids.to(device)\n",
    "    img_feats = img_feats.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        img_att = encoder(img_feats)\n",
    "        tmp_eval_loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids,\n",
    "                              input_mask, s2_input_mask, added_input_mask, label_ids)\n",
    "        logits = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                       s2_input_mask, added_input_mask)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    true_label_list.append(label_ids)\n",
    "    pred_label_list.append(logits)\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "loss = tr_loss / nb_tr_steps if args.do_train else None\n",
    "true_label = np.concatenate(true_label_list)\n",
    "pred_outputs = np.concatenate(pred_label_list)\n",
    "precision, recall, F_score = macro_f1(true_label, pred_outputs)\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'precision': precision,\n",
    "          'recall': recall,\n",
    "          'f_score': F_score,\n",
    "          'global_step': global_step,\n",
    "          'loss': loss}\n",
    "pred_label = np.argmax(pred_outputs, axis=-1)\n",
    "fout_p = open(os.path.join(args.output_dir, \"pred.txt\"), 'w')\n",
    "fout_t = open(os.path.join(args.output_dir, \"true.txt\"), 'w')\n",
    "for i in range(len(pred_label)):\n",
    "    attstr = str(pred_label[i])\n",
    "    fout_p.write(attstr + '\\n')\n",
    "for i in range(len(true_label)):\n",
    "    attstr = str(true_label[i])\n",
    "    fout_t.write(attstr + '\\n')\n",
    "output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Test Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "fout_p.close()\n",
    "fout_t.close()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
