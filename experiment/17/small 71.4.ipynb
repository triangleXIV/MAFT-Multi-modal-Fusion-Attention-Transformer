{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a53601-3667-489d-8763-ef88e06264c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: yacs in /usr/local/miniconda3/lib/python3.8/site-packages (0.1.8)\n",
      "Requirement already satisfied: timm in /usr/local/miniconda3/lib/python3.8/site-packages (0.9.11)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/miniconda3/lib/python3.8/site-packages (0.1.99)\n",
      "Requirement already satisfied: regex in /usr/local/miniconda3/lib/python3.8/site-packages (2023.10.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/miniconda3/lib/python3.8/site-packages (from yacs) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.19.4)\n",
      "Requirement already satisfied: torch>=1.7 in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (2.0.0+cu117)\n",
      "Requirement already satisfied: torchvision in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.15.1+cu117)\n",
      "Requirement already satisfied: safetensors in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.4.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.0)\n",
      "Requirement already satisfied: sympy in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.9.0)\n",
      "Requirement already satisfied: lit in /usr/local/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (15.0.7)\n",
      "Requirement already satisfied: cmake in /usr/local/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (23.0)\n",
      "Requirement already satisfied: requests in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2.29.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2023.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from torchvision->timm) (9.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/miniconda3/lib/python3.8/site-packages (from torchvision->timm) (1.24.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (1.25.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/miniconda3/lib/python3.8/site-packages (from sympy->torch>=1.7->timm) (1.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yacs timm sentencepiece regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889c919a-b4e7-40c7-acc4-e14f59053eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from yacs.config import CfgNode as CN\n",
    "from tqdm import tqdm,trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from toolcls import AbmsaProcessor,seed_everything,convert_mm_examples_to_features,BertConfig\n",
    "from models.swin.swintransformer import SwinTransformer,get_config\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from models.deberta.spm_tokenizer import SPMTokenizer\n",
    "from models.deberta.deberta import SwinBERTa\n",
    "from models.logs import logger\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import safetensors.torch\n",
    "    _has_safetensors = True\n",
    "except ImportError:\n",
    "    _has_safetensors = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b10da-e9f2-4d6c-bffc-ae6e5b6fd01f",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cec1ab2-a125-4daa-8ee0-bb0c24fca243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img_encoder config\n",
    "_C = CN()\n",
    "config = _C.clone()\n",
    "config.LOCAL_RANK = -1\n",
    "\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return (1-x)/(1-warmup)\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    preds = np.argmax(y_pred, axis=-1)\n",
    "    true = y_true\n",
    "    p_macro, r_macro, f_macro, support_macro \\\n",
    "      = precision_recall_fscore_support(true, preds, average='macro')\n",
    "    #f_macro = 2*p_macro*r_macro/(p_macro+r_macro)\n",
    "    return p_macro, r_macro, f_macro\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "## Required parameters\n",
    "parser.add_argument(\"--data_dir\",default='./twitterdataset/absa_data/twitter',type=str,#文本数据位置\n",
    "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "parser.add_argument(\"--img_ckpt\", default='./pretrains/swin_small_patch4_window7_224_1k.pth', type=str,#swin预训练模型的准确位置\n",
    "                    help=\"Bert pre-trained model selected in the list: S24-224,S24-336 \")\n",
    "parser.add_argument(\"--model_name_or_path\", default='./pretrains', type=str,#albert预训练模型存放的文件夹\n",
    "                    help=\"Path to pre-trained model or shortcut name selected in the list\")\n",
    "parser.add_argument(\"--task_name\",default='twitter17',type=str,#要加载哪个数据集 Twitter是17 Twitter15是15\n",
    "                    help=\"The name of the task to train.\")\n",
    "parser.add_argument(\"--output_dir\",default='./output',type=str,\n",
    "                    help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "parser.add_argument('--path_image', default='./twitterdataset/img_data/twitter2017_images',#图像的位置\n",
    "                    help='path to images')\n",
    "parser.add_argument('--init_model',\n",
    "                    type=str,\n",
    "                    default='./pretrains/pytorch_model_small.bin',\n",
    "                    help=\"The model state file used to initialize the model weights.\")\n",
    "parser.add_argument('--model_config',\n",
    "                    type=str,\n",
    "                    default='./pretrains/config_small.json',\n",
    "                    help=\"The config file of bert model.\")\n",
    "parser.add_argument('--pre_trained',\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    help=\"The path of pre-trained RoBERTa model\")\n",
    "parser.add_argument('--vocab_path',\n",
    "                    default='./pretrains/spm.model',\n",
    "                    type=str,\n",
    "                    help=\"The path of the vocabulary\")\n",
    "## Other parameters\n",
    "parser.add_argument('--crop_size', type=int, default=224, help='crop size of image')\n",
    "parser.add_argument(\"--max_seq_length\",default=64,type=int,\n",
    "                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--max_entity_length\",default=16,type=int,\n",
    "                    help=\"The maximum entity input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--do_train\",action='store_true',default=True,\n",
    "                    help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_lower_case\",action='store_true',default=True,\n",
    "                    help=\"Set this flag if you are using an uncased model.\")\n",
    "parser.add_argument(\"--train_batch_size\",default=24,type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\",default=24,type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--learning_rate\",default=3e-5,type=float,#可能会暴毙\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\",default=8.0,type=float,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\",default=0.1,type=float,\n",
    "                    help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                         \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",action='store_true',default=False,\n",
    "                    help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",type=int,default=-1,\n",
    "                    help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed',type=int,default=3,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument('--gradient_accumulation_steps',type=int,default=1,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument('--fp16',default=False,action='store_true',#kaggle没搞懂怎么导入apex 索性就32位吧\n",
    "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--loss_scale',type=float, default=0,\n",
    "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                         \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                         \"Positive power of 2: static loss scaling value.\\n\")\n",
    "parser.add_argument('--overwrite_output_dir', action='store_true',default=True,#是否覆盖原本的输出文件\n",
    "                    help=\"Overwrite the content of the output directory\")\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument('--cfg', type=str, default=\"./pretrains/swin_small_patch4_window7_224.yaml\", metavar=\"FILE\",\n",
    "                    help='path to config file', )\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e4b1e-0876-45c8-ba8d-f362f357e2f8",
   "metadata": {},
   "source": [
    "# 基础随机数 cuda 路径之类的东西配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b7ddc1-de3f-448d-8a66-3e80053920a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.task_name == \"twitter17\":\n",
    "    args.path_image = \"./twitterdataset/img_data/twitter2017_images\"\n",
    "elif args.task_name == \"twitter15\":\n",
    "    args.path_image = \"./twitterdataset/img_data/twitter2015_images\"\n",
    "else:\n",
    "    print(\"The task name is not right!\")\n",
    "processors = {\n",
    "        \"twitter15\": AbmsaProcessor,    # our twitter-2015 dataset\n",
    "        \"twitter17\": AbmsaProcessor         # our twitter-2017 dataset\n",
    "}\n",
    "num_labels_task = {\n",
    "    \"twitter15\": 3,                # our twitter-2015 dataset\n",
    "    \"twitter17\": 3                     # our twitter-2017 dataset\n",
    "}\n",
    "seed_everything(args.seed) #固定随机数种子\n",
    "task_name = args.task_name.lower()\n",
    "#初始化输出的文件夹\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "args.output_dir = args.output_dir\n",
    "if os.path.exists(args.output_dir) and os.listdir(\n",
    "        args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir))\n",
    "#设置cuda\n",
    "if config.LOCAL_RANK == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(config.LOCAL_RANK)\n",
    "    device = torch.device(\"cuda\", config.LOCAL_RANK)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "processor = processors[task_name]()#获得读取tsv文件方法\n",
    "num_labels = num_labels_task[task_name]#判定几分类\n",
    "label_list = processor.get_labels()#获得分类标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bb07a-89a6-4764-831f-b0fd96519557",
   "metadata": {},
   "source": [
    "# 定义模型 载入预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d989f45-96e0-49fe-b181-039aeca4cb67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 15:03:21,817 - root - INFO - Loaded pretrained model file ./pretrains/pytorch_model_small.bin\n",
      "2023-11-21 15:03:22,929 - root - WARNING - Missing keys: [], unexpected_keys: [], error_msgs: []\n",
      "2023-11-21 15:03:23,077 - root - INFO - Loaded pretrained model file ./pretrains/pytorch_model_small.bin\n",
      "2023-11-21 15:03:24,162 - root - WARNING - Missing keys: [], unexpected_keys: [], error_msgs: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from ./pretrains/swin_small_patch4_window7_224.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.026)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.039)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.052)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.065)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.104)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.117)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.130)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.143)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.157)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.170)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.183)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.196)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.209)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.222)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.235)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.248)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.261)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.274)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.287)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.300)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = os.path.join(args.model_name_or_path, 'bert_config.json')\n",
    "bert_config = BertConfig.from_json_file(config_file)\n",
    "# 读取词表 方便之后把文字转数字id 返回的是一个30000词字典\n",
    "tokenizer = SPMTokenizer(args.vocab_path)\n",
    "#创建并导入预训练模型 这个模型用于文本encoder 融合图文feature\n",
    "model=SwinBERTa(args, bert_config,args.init_model)\n",
    "model.to(device)\n",
    "#设定图像encoder \n",
    "config = get_config(args)\n",
    "encoder = SwinTransformer(img_size=224,\n",
    "                          patch_size=config.MODEL.SWIN.PATCH_SIZE,\n",
    "                          in_chans=config.MODEL.SWIN.IN_CHANS,\n",
    "                          num_classes=config.MODEL.NUM_CLASSES,\n",
    "                          embed_dim=config.MODEL.SWIN.EMBED_DIM,\n",
    "                          depths=config.MODEL.SWIN.DEPTHS,\n",
    "                          num_heads=config.MODEL.SWIN.NUM_HEADS,\n",
    "                          window_size=config.MODEL.SWIN.WINDOW_SIZE,\n",
    "                          mlp_ratio=config.MODEL.SWIN.MLP_RATIO,\n",
    "                          qkv_bias=config.MODEL.SWIN.QKV_BIAS,\n",
    "                          qk_scale=config.MODEL.SWIN.QK_SCALE,\n",
    "                          drop_rate=config.MODEL.DROP_RATE,\n",
    "                          drop_path_rate=config.MODEL.DROP_PATH_RATE,\n",
    "                          patch_norm=config.MODEL.SWIN.PATCH_NORM,\n",
    "                          use_checkpoint=False)\n",
    "pretrained_dict = torch.load(args.img_ckpt, map_location='cpu')\n",
    "pretrained_dict = pretrained_dict['model']\n",
    "unexpected_keys = {\"head.weight\", \"head.bias\"}\n",
    "# 删除不匹配的键值\n",
    "for key in unexpected_keys:\n",
    "    del pretrained_dict[key]\n",
    "missing_keys, unexpected_keys = encoder.load_state_dict(pretrained_dict, strict=False)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d60ab-7d42-45bd-bf0f-4df840e90163",
   "metadata": {},
   "source": [
    "# 加载优化器 pth保存路径配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdb8aab-870e-4951-914b-77dd9117c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 15:03:27,314 - root - INFO - LOOKING AT ./twitterdataset/absa_data/twitter/train.tsv\n"
     ]
    }
   ],
   "source": [
    "train_examples = processor.get_train_examples(args.data_dir)#获取训练集文本内容\n",
    "eval_examples = processor.get_dev_examples(args.data_dir)#获取验证集文本内容\n",
    "num_train_steps = int(len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
    "t_total = num_train_steps\n",
    "#文本和融合部分参数的优化策略\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "#图像部分参数优化策略\n",
    "def check_keywords_in_name(name, keywords=()):\n",
    "    isin = False\n",
    "    for keyword in keywords:\n",
    "        if keyword in name:\n",
    "            isin = True\n",
    "    return isin\n",
    "def set_weight_decay(model, skip_list=(), skip_keywords=()):\n",
    "    has_decay = []\n",
    "    no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n",
    "                check_keywords_in_name(name, skip_keywords):\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            has_decay.append(param)\n",
    "    return [{'params': has_decay,'weight_decay': 0.01},\n",
    "            {'params': no_decay, 'weight_decay': 0.}]\n",
    "skip = {'absolute_pos_embed'}\n",
    "skip_keywords = {'relative_position_bias_table'}\n",
    "optimizer_grouped_parameters2 = set_weight_decay(encoder, skip, skip_keywords)\n",
    "#合并两组参数统一传入adamw优化器调参\n",
    "optimizer_grouped_parameters = optimizer_grouped_parameters1 + optimizer_grouped_parameters2\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate,eps=1e-6)\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "t_total = num_train_steps\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=int(t_total * args.warmup_proportion), t_total=t_total)\n",
    "output_model_file = os.path.join(args.output_dir, \"pytorch_model.pth\")\n",
    "output_encoder_file = os.path.join(args.output_dir, \"pytorch_encoder.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0741964-7b04-4bec-a153-b4506af1fde7",
   "metadata": {},
   "source": [
    "# 载入训练集 验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4570d1-b61a-4e0e-8c8b-ac2e3e86103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 15:03:27,383 - root - INFO - *** Example ***\n",
      "2023-11-21 15:03:27,384 - root - INFO - guid: train-1\n",
      "2023-11-21 15:03:27,385 - root - INFO - tokens: [CLS] ▁how ▁$ t $ ▁is ▁changing ▁the ▁influencer ▁game ▁: [SEP] ▁jake ▁paul [SEP]\n",
      "2023-11-21 15:03:27,386 - root - INFO - input_ids: 1 361 419 297 1814 269 2198 262 29655 522 877 2 109757 38723 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 15:03:27,387 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 15:03:27,388 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 15:03:27,389 - root - INFO - label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 134\n",
      "the max length of sentence a: 49 entity b: 10 total length: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 15:03:54,533 - root - INFO - *** Example ***\n",
      "2023-11-21 15:03:54,534 - root - INFO - guid: dev-1\n",
      "2023-11-21 15:03:54,535 - root - INFO - tokens: [CLS] ▁looking ▁forward ▁to ▁the ▁$ t $ ▁from ▁4 ▁- ▁8 ▁july ▁! ▁more ▁info ▁here ▁# ▁heritage ▁# ▁music [SEP] ▁f other ing hay ▁festival [SEP]\n",
      "2023-11-21 15:03:54,536 - root - INFO - input_ids: 1 562 939 264 262 419 297 1814 292 453 341 578 52434 1084 310 2470 422 953 5456 953 755 2 2994 10705 510 28577 3694 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 15:03:54,537 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 15:03:54,537 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 15:03:54,538 - root - INFO - label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 29\n",
      "the max length of sentence a: 48 entity b: 10 total length: 55\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_mm_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "            args.path_image)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in train_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in train_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in train_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in train_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                           all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids,\n",
    "                           all_img_feats, all_label_ids)\n",
    "if args.local_rank == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size,\n",
    "                              drop_last=True)\n",
    "# 获取验证集\n",
    "eval_features = convert_mm_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "    args.path_image)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in eval_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                          all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids, \\\n",
    "                          all_img_feats, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size, drop_last=True)\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "max_acc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2576d-968e-4f67-b827-48e0335ebc75",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a3d005a-22b4-48e1-bb7e-70278a137982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 15:04:03,692 - root - INFO - *************** Running training ***************\n",
      "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]2023-11-21 15:04:03,696 - root - INFO - ********** Epoch: 0 **********\n",
      "2023-11-21 15:04:03,697 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 15:04:03,698 - root - INFO -   Batch size = 24\n",
      "2023-11-21 15:04:03,699 - root - INFO -   Num steps = 1187\n",
      "Iteration:   0%|          | 0/148 [00:00<?, ?it/s]/usr/local/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Iteration (loss: 0.9341,lr:0.0000299417): 100%|██████████| 148/148 [00:44<00:00,  3.32it/s]\n",
      "2023-11-21 15:04:48,335 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 15:04:48,336 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 15:04:48,337 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.5981): 100%|██████████| 49/49 [00:05<00:00,  8.41it/s]\n",
      "2023-11-21 15:04:54,176 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 15:04:54,177 - root - INFO -   eval_accuracy = 0.6045918367346939\n",
      "2023-11-21 15:04:54,178 - root - INFO -   eval_loss = 0.8445291579986105\n",
      "2023-11-21 15:04:54,179 - root - INFO -   f_score = 0.5726397802950451\n",
      "2023-11-21 15:04:54,179 - root - INFO -   global_step = 148\n",
      "2023-11-21 15:04:54,180 - root - INFO -   loss = 1.0007565025542233\n",
      "Epoch:  12%|█▎        | 1/8 [00:53<06:14, 53.46s/it]2023-11-21 15:04:57,157 - root - INFO - ********** Epoch: 1 **********\n",
      "2023-11-21 15:04:57,159 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 15:04:57,160 - root - INFO -   Batch size = 24\n",
      "2023-11-21 15:04:57,161 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.6666,lr:0.0000279941): 100%|██████████| 148/148 [00:44<00:00,  3.35it/s]\n",
      "2023-11-21 15:05:41,334 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 15:05:41,335 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 15:05:41,337 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.3933): 100%|██████████| 49/49 [00:05<00:00,  8.36it/s]\n",
      "2023-11-21 15:05:47,214 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 15:05:47,215 - root - INFO -   eval_accuracy = 0.6530612244897959\n",
      "2023-11-21 15:05:47,215 - root - INFO -   eval_loss = 0.7484074447836194\n",
      "2023-11-21 15:05:47,216 - root - INFO -   f_score = 0.623260935971447\n",
      "2023-11-21 15:05:47,217 - root - INFO -   global_step = 296\n",
      "2023-11-21 15:05:47,218 - root - INFO -   loss = 0.7008213529715667\n",
      "Epoch:  25%|██▌       | 2/8 [01:46<05:19, 53.18s/it]2023-11-21 15:05:50,142 - root - INFO - ********** Epoch: 2 **********\n",
      "2023-11-21 15:05:50,144 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 15:05:50,145 - root - INFO -   Batch size = 24\n",
      "2023-11-21 15:05:50,146 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.2697,lr:0.0000236267): 100%|██████████| 148/148 [00:43<00:00,  3.36it/s]\n",
      "2023-11-21 15:06:34,138 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 15:06:34,139 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 15:06:34,140 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.5467): 100%|██████████| 49/49 [00:05<00:00,  8.38it/s]\n",
      "2023-11-21 15:06:40,004 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 15:06:40,005 - root - INFO -   eval_accuracy = 0.6675170068027211\n",
      "2023-11-21 15:06:40,006 - root - INFO -   eval_loss = 0.8764555533321536\n",
      "2023-11-21 15:06:40,007 - root - INFO -   f_score = 0.6460815009952176\n",
      "2023-11-21 15:06:40,008 - root - INFO -   global_step = 444\n",
      "2023-11-21 15:06:40,009 - root - INFO -   loss = 0.44890398824134387\n",
      "Epoch:  38%|███▊      | 3/8 [02:39<04:25, 53.01s/it]2023-11-21 15:06:42,943 - root - INFO - ********** Epoch: 3 **********\n",
      "2023-11-21 15:06:42,944 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 15:06:42,945 - root - INFO -   Batch size = 24\n",
      "2023-11-21 15:06:42,945 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.0865,lr:0.0000176529): 100%|██████████| 148/148 [00:43<00:00,  3.38it/s]\n",
      "2023-11-21 15:07:26,713 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 15:07:26,714 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 15:07:26,715 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.8227): 100%|██████████| 49/49 [00:05<00:00,  8.39it/s]\n",
      "2023-11-21 15:07:32,570 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 15:07:32,571 - root - INFO -   eval_accuracy = 0.6777210884353742\n",
      "2023-11-21 15:07:32,572 - root - INFO -   eval_loss = 1.0454664449302518\n",
      "2023-11-21 15:07:32,573 - root - INFO -   f_score = 0.659309251262654\n",
      "2023-11-21 15:07:32,574 - root - INFO -   global_step = 592\n",
      "2023-11-21 15:07:32,575 - root - INFO -   loss = 0.26828656828886754\n",
      "Epoch:  50%|█████     | 4/8 [03:31<03:31, 52.81s/it]2023-11-21 15:07:35,462 - root - INFO - ********** Epoch: 4 **********\n",
      "2023-11-21 15:07:35,463 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 15:07:35,465 - root - INFO -   Batch size = 24\n",
      "2023-11-21 15:07:35,466 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.0213,lr:0.0000111852): 100%|██████████| 148/148 [00:44<00:00,  3.34it/s]\n",
      "2023-11-21 15:08:19,805 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 15:08:19,806 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 15:08:19,807 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.7989): 100%|██████████| 49/49 [00:05<00:00,  8.28it/s]\n",
      "2023-11-21 15:08:25,745 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 15:08:25,746 - root - INFO -   eval_accuracy = 0.6785714285714286\n",
      "2023-11-21 15:08:25,746 - root - INFO -   eval_loss = 1.2635852788784065\n",
      "2023-11-21 15:08:25,747 - root - INFO -   f_score = 0.6491281700375144\n",
      "2023-11-21 15:08:25,748 - root - INFO -   global_step = 740\n",
      "2023-11-21 15:08:25,749 - root - INFO -   loss = 0.11192642030230648\n",
      "Epoch:  62%|██████▎   | 5/8 [04:24<02:38, 52.96s/it]2023-11-21 15:08:28,679 - root - INFO - ********** Epoch: 5 **********\n",
      "2023-11-21 15:08:28,680 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 15:08:28,681 - root - INFO -   Batch size = 24\n",
      "2023-11-21 15:08:28,682 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.0144,lr:0.0000054278): 100%|██████████| 148/148 [00:43<00:00,  3.41it/s]\n",
      "2023-11-21 15:09:12,123 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 15:09:12,124 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 15:09:12,125 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 1.0226): 100%|██████████| 49/49 [00:05<00:00,  8.36it/s]\n",
      "2023-11-21 15:09:18,002 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 15:09:18,003 - root - INFO -   eval_accuracy = 0.6836734693877551\n",
      "2023-11-21 15:09:18,004 - root - INFO -   eval_loss = 1.450114438424305\n",
      "2023-11-21 15:09:18,005 - root - INFO -   f_score = 0.657258168528927\n",
      "2023-11-21 15:09:18,006 - root - INFO -   global_step = 888\n",
      "2023-11-21 15:09:18,007 - root - INFO -   loss = 0.05387361185480463\n",
      "Epoch:  75%|███████▌  | 6/8 [05:17<01:45, 52.68s/it]2023-11-21 15:09:20,809 - root - INFO - ********** Epoch: 6 **********\n",
      "2023-11-21 15:09:20,811 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 15:09:20,812 - root - INFO -   Batch size = 24\n",
      "2023-11-21 15:09:20,813 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.0093,lr:0.0000014529): 100%|██████████| 148/148 [00:43<00:00,  3.38it/s]\n",
      "2023-11-21 15:10:04,625 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 15:10:04,626 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 15:10:04,627 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 1.0915): 100%|██████████| 49/49 [00:05<00:00,  8.37it/s]\n",
      "2023-11-21 15:10:10,500 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 15:10:10,500 - root - INFO -   eval_accuracy = 0.6879251700680272\n",
      "2023-11-21 15:10:10,501 - root - INFO -   eval_loss = 1.4546469027290538\n",
      "2023-11-21 15:10:10,502 - root - INFO -   f_score = 0.6648336184913423\n",
      "2023-11-21 15:10:10,503 - root - INFO -   global_step = 1036\n",
      "2023-11-21 15:10:10,504 - root - INFO -   loss = 0.030925584956217004\n",
      "Epoch:  88%|████████▊ | 7/8 [06:09<00:52, 52.67s/it]2023-11-21 15:10:13,472 - root - INFO - ********** Epoch: 7 **********\n",
      "2023-11-21 15:10:13,473 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 15:10:13,474 - root - INFO -   Batch size = 24\n",
      "2023-11-21 15:10:13,476 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.0450,lr:0.0000000006): 100%|██████████| 148/148 [00:43<00:00,  3.36it/s]\n",
      "2023-11-21 15:10:57,479 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 15:10:57,480 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 15:10:57,481 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 1.1028): 100%|██████████| 49/49 [00:05<00:00,  8.35it/s]\n",
      "2023-11-21 15:11:03,368 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 15:11:03,369 - root - INFO -   eval_accuracy = 0.6887755102040817\n",
      "2023-11-21 15:11:03,370 - root - INFO -   eval_loss = 1.4747865531517534\n",
      "2023-11-21 15:11:03,371 - root - INFO -   f_score = 0.6630130814372424\n",
      "2023-11-21 15:11:03,372 - root - INFO -   global_step = 1184\n",
      "2023-11-21 15:11:03,373 - root - INFO -   loss = 0.02285116350292417\n",
      "Epoch: 100%|██████████| 8/8 [07:02<00:00, 52.83s/it]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"*************** Running training ***************\")\n",
    "for train_idx in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "    logger.info(\"********** Epoch: \" + str(train_idx) + \" **********\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "    model.train()\n",
    "    encoder.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), desc=\"Iteration\", total=len(train_dataloader), position=0)\n",
    "    for step, batch in progress_bar:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "            img_feats, label_ids = batch\n",
    "        img_att = encoder(img_feats)\n",
    "        loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                         s2_input_mask, \\\n",
    "                         added_input_mask, label_ids)\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        scheduler.step()  # 使用学习率调整算法\n",
    "        for param_group in optimizer.param_groups:\n",
    "            progress_bar.set_description(f\"Iteration (loss: {loss.item():.4f},lr:{param_group['lr']:.10f})\")\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    logger.info(\"***** Running evaluation on Dev Set*****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    model.eval()\n",
    "    encoder.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_label_list = []\n",
    "    pred_label_list = []\n",
    "    #验证\n",
    "    progress_bar = tqdm(eval_dataloader, desc=\"Evaluating\", position=0)\n",
    "    for input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "            img_feats, label_ids in progress_bar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        added_input_mask = added_input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        s2_input_ids = s2_input_ids.to(device)\n",
    "        s2_input_mask = s2_input_mask.to(device)\n",
    "        s2_segment_ids = s2_segment_ids.to(device)\n",
    "        img_feats = img_feats.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            img_att = encoder(img_feats)\n",
    "            tmp_eval_loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids,\n",
    "                                  input_mask, s2_input_mask, added_input_mask, label_ids)\n",
    "            logits = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                           s2_input_mask, added_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        true_label_list.append(label_ids)\n",
    "        pred_label_list.append(logits)\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        progress_bar.set_description(f\"Evaluating (tmp_eval_loss: {tmp_eval_loss.item():.4f})\")\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    loss = tr_loss / nb_tr_steps if args.do_train else None\n",
    "    true_label = np.concatenate(true_label_list)\n",
    "    pred_outputs = np.concatenate(pred_label_list)\n",
    "    precision, recall, F_score = macro_f1(true_label, pred_outputs)\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "              'f_score': F_score,\n",
    "              'global_step': global_step,\n",
    "              'loss': loss}\n",
    "    logger.info(\"***** Dev Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "    if eval_accuracy >= max_acc:\n",
    "        # Save a trained model\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        encoder_to_save = encoder.module if hasattr(encoder,\n",
    "                                                    'module') else encoder  # Only save the model it-self\n",
    "        if args.do_train:\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "            torch.save(encoder_to_save.state_dict(), output_encoder_file)\n",
    "        max_acc = eval_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa03d8-dfde-4274-8d3f-b544be05ea97",
   "metadata": {},
   "source": [
    "# 取最好的结果在测试集上验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec52fd4-39df-45aa-a364-7d2c68529d65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 15:11:07,514 - root - INFO - *** Example ***\n",
      "2023-11-21 15:11:07,515 - root - INFO - guid: test-1\n",
      "2023-11-21 15:11:07,516 - root - INFO - tokens: [CLS] ▁# ▁$ t $ ▁performs ▁at ▁stagecoach ▁# ▁music festival ▁2016 [SEP] ▁sam hunt [SEP]\n",
      "2023-11-21 15:11:07,517 - root - INFO - input_ids: 1 953 419 297 1814 8993 288 109755 953 755 47550 892 2 20782 39396 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 15:11:07,518 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 15:11:07,519 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 15:11:07,520 - root - INFO - label: 2 (id = 2)\n",
      "2023-11-21 15:11:16,800 - root - INFO - ***** Running evaluation on Test Set*****\n",
      "2023-11-21 15:11:16,801 - root - INFO -   Num examples = 1234\n",
      "2023-11-21 15:11:16,802 - root - INFO -   Batch size = 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 62\n",
      "the max length of sentence a: 71 entity b: 10 total length: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 52/52 [00:06<00:00,  8.50it/s]\n",
      "2023-11-21 15:11:23,136 - root - INFO - ***** Test Eval results *****\n",
      "2023-11-21 15:11:23,137 - root - INFO -   eval_accuracy = 0.7147487844408428\n",
      "2023-11-21 15:11:23,138 - root - INFO -   eval_loss = 1.3761816554917738\n",
      "2023-11-21 15:11:23,139 - root - INFO -   f_score = 0.6971758783810332\n",
      "2023-11-21 15:11:23,140 - root - INFO -   global_step = 1184\n",
      "2023-11-21 15:11:23,141 - root - INFO -   loss = 0.02285116350292417\n",
      "2023-11-21 15:11:23,142 - root - INFO -   precision = 0.7113463465723191\n",
      "2023-11-21 15:11:23,143 - root - INFO -   recall = 0.6862735826713185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3761816554917738, 'eval_accuracy': 0.7147487844408428, 'precision': 0.7113463465723191, 'recall': 0.6862735826713185, 'f_score': 0.6971758783810332, 'global_step': 1184, 'loss': 0.02285116350292417}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # 先把cuda清空了\n",
    "model.load_state_dict(torch.load(output_model_file))#载入eval最好的结果\n",
    "encoder.load_state_dict(torch.load(output_encoder_file))#载入eval最好的结果\n",
    "eval_examples = processor.get_test_examples(args.data_dir)#获得测试集\n",
    "eval_features = convert_mm_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "    args.path_image)\n",
    "logger.info(\"***** Running evaluation on Test Set*****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in eval_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                          all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids,\n",
    "                          all_img_feats, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "model.eval()\n",
    "encoder.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "true_label_list = []\n",
    "pred_label_list = []\n",
    "for input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "        img_feats, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    added_input_mask = added_input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    s2_input_ids = s2_input_ids.to(device)\n",
    "    s2_input_mask = s2_input_mask.to(device)\n",
    "    s2_segment_ids = s2_segment_ids.to(device)\n",
    "    img_feats = img_feats.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        img_att = encoder(img_feats)\n",
    "        tmp_eval_loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids,\n",
    "                              input_mask, s2_input_mask, added_input_mask, label_ids)\n",
    "        logits = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                       s2_input_mask, added_input_mask)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    true_label_list.append(label_ids)\n",
    "    pred_label_list.append(logits)\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "loss = tr_loss / nb_tr_steps if args.do_train else None\n",
    "true_label = np.concatenate(true_label_list)\n",
    "pred_outputs = np.concatenate(pred_label_list)\n",
    "precision, recall, F_score = macro_f1(true_label, pred_outputs)\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'precision': precision,\n",
    "          'recall': recall,\n",
    "          'f_score': F_score,\n",
    "          'global_step': global_step,\n",
    "          'loss': loss}\n",
    "pred_label = np.argmax(pred_outputs, axis=-1)\n",
    "fout_p = open(os.path.join(args.output_dir, \"pred.txt\"), 'w')\n",
    "fout_t = open(os.path.join(args.output_dir, \"true.txt\"), 'w')\n",
    "for i in range(len(pred_label)):\n",
    "    attstr = str(pred_label[i])\n",
    "    fout_p.write(attstr + '\\n')\n",
    "for i in range(len(true_label)):\n",
    "    attstr = str(true_label[i])\n",
    "    fout_t.write(attstr + '\\n')\n",
    "output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Test Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "fout_p.close()\n",
    "fout_t.close()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15201130-50fe-4593-86cc-6f3c4b3b17b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
