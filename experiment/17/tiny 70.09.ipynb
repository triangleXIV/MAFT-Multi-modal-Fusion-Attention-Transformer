{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a53601-3667-489d-8763-ef88e06264c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: yacs in /usr/local/miniconda3/lib/python3.8/site-packages (0.1.8)\n",
      "Requirement already satisfied: timm in /usr/local/miniconda3/lib/python3.8/site-packages (0.9.11)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/miniconda3/lib/python3.8/site-packages (0.1.99)\n",
      "Requirement already satisfied: regex in /usr/local/miniconda3/lib/python3.8/site-packages (2023.10.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/miniconda3/lib/python3.8/site-packages (from yacs) (6.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.4.0)\n",
      "Requirement already satisfied: torch>=1.7 in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (2.0.0+cu117)\n",
      "Requirement already satisfied: torchvision in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.15.1+cu117)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.19.4)\n",
      "Requirement already satisfied: filelock in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (2.0.0)\n",
      "Requirement already satisfied: sympy in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (1.11.1)\n",
      "Requirement already satisfied: cmake in /usr/local/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (15.0.7)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (23.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2023.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2.29.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from torchvision->timm) (9.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/miniconda3/lib/python3.8/site-packages (from torchvision->timm) (1.24.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/miniconda3/lib/python3.8/site-packages (from sympy->torch>=1.7->timm) (1.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yacs timm sentencepiece regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889c919a-b4e7-40c7-acc4-e14f59053eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from yacs.config import CfgNode as CN\n",
    "from tqdm import tqdm,trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from toolcls import AbmsaProcessor,seed_everything,convert_mm_examples_to_features,BertConfig\n",
    "from models.swin.swintransformer import SwinTransformer,get_config\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from models.deberta.spm_tokenizer import SPMTokenizer\n",
    "from models.deberta.deberta import SwinBERTa\n",
    "from models.logs import logger\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import safetensors.torch\n",
    "    _has_safetensors = True\n",
    "except ImportError:\n",
    "    _has_safetensors = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b10da-e9f2-4d6c-bffc-ae6e5b6fd01f",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cec1ab2-a125-4daa-8ee0-bb0c24fca243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img_encoder config\n",
    "_C = CN()\n",
    "config = _C.clone()\n",
    "config.LOCAL_RANK = -1\n",
    "\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return (1-x)/(1-warmup)\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    preds = np.argmax(y_pred, axis=-1)\n",
    "    true = y_true\n",
    "    p_macro, r_macro, f_macro, support_macro \\\n",
    "      = precision_recall_fscore_support(true, preds, average='macro')\n",
    "    #f_macro = 2*p_macro*r_macro/(p_macro+r_macro)\n",
    "    return p_macro, r_macro, f_macro\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "## Required parameters\n",
    "parser.add_argument(\"--data_dir\",default='./twitterdataset/absa_data/twitter',type=str,#文本数据位置\n",
    "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "parser.add_argument(\"--img_ckpt\", default='./pretrains/swin_tiny_patch4_window7_224_1k.pth', type=str,#swin预训练模型的准确位置\n",
    "                    help=\"Bert pre-trained model selected in the list: S24-224,S24-336 \")\n",
    "parser.add_argument(\"--model_name_or_path\", default='./pretrains', type=str,#albert预训练模型存放的文件夹\n",
    "                    help=\"Path to pre-trained model or shortcut name selected in the list\")\n",
    "parser.add_argument(\"--task_name\",default='twitter17',type=str,#要加载哪个数据集 Twitter是17 Twitter15是15\n",
    "                    help=\"The name of the task to train.\")\n",
    "parser.add_argument(\"--output_dir\",default='./output',type=str,\n",
    "                    help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "parser.add_argument('--path_image', default='./twitterdataset/img_data/twitter2017_images',#图像的位置\n",
    "                    help='path to images')\n",
    "parser.add_argument('--init_model',\n",
    "                    type=str,\n",
    "                    default='./pretrains/pytorch_model_tiny.bin',\n",
    "                    help=\"The model state file used to initialize the model weights.\")\n",
    "parser.add_argument('--model_config',\n",
    "                    type=str,\n",
    "                    default='./pretrains/config_tiny.json',\n",
    "                    help=\"The config file of bert model.\")\n",
    "parser.add_argument('--pre_trained',\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    help=\"The path of pre-trained RoBERTa model\")\n",
    "parser.add_argument('--vocab_path',\n",
    "                    default='./pretrains/spm.model',\n",
    "                    type=str,\n",
    "                    help=\"The path of the vocabulary\")\n",
    "## Other parameters\n",
    "parser.add_argument('--crop_size', type=int, default=224, help='crop size of image')\n",
    "parser.add_argument(\"--max_seq_length\",default=64,type=int,\n",
    "                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--max_entity_length\",default=16,type=int,\n",
    "                    help=\"The maximum entity input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--do_train\",action='store_true',default=True,\n",
    "                    help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_lower_case\",action='store_true',default=True,\n",
    "                    help=\"Set this flag if you are using an uncased model.\")\n",
    "parser.add_argument(\"--train_batch_size\",default=24,type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\",default=24,type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--learning_rate\",default=3e-5,type=float,#可能会暴毙\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\",default=8.0,type=float,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\",default=0.1,type=float,\n",
    "                    help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                         \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",action='store_true',default=False,\n",
    "                    help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",type=int,default=-1,\n",
    "                    help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed',type=int,default=8,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument('--gradient_accumulation_steps',type=int,default=1,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument('--fp16',default=False,action='store_true',#kaggle没搞懂怎么导入apex 索性就32位吧\n",
    "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--loss_scale',type=float, default=0,\n",
    "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                         \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                         \"Positive power of 2: static loss scaling value.\\n\")\n",
    "parser.add_argument('--overwrite_output_dir', action='store_true',default=True,#是否覆盖原本的输出文件\n",
    "                    help=\"Overwrite the content of the output directory\")\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument('--cfg', type=str, default=\"./pretrains/swin_tiny_patch4_window7_224.yaml\", metavar=\"FILE\",\n",
    "                    help='path to config file', )\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e4b1e-0876-45c8-ba8d-f362f357e2f8",
   "metadata": {},
   "source": [
    "# 基础随机数 cuda 路径之类的东西配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b7ddc1-de3f-448d-8a66-3e80053920a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.task_name == \"twitter17\":\n",
    "    args.path_image = \"./twitterdataset/img_data/twitter2017_images\"\n",
    "elif args.task_name == \"twitter15\":\n",
    "    args.path_image = \"./twitterdataset/img_data/twitter2015_images\"\n",
    "else:\n",
    "    print(\"The task name is not right!\")\n",
    "processors = {\n",
    "        \"twitter15\": AbmsaProcessor,    # our twitter-2015 dataset\n",
    "        \"twitter17\": AbmsaProcessor         # our twitter-2017 dataset\n",
    "}\n",
    "num_labels_task = {\n",
    "    \"twitter15\": 3,                # our twitter-2015 dataset\n",
    "    \"twitter17\": 3                     # our twitter-2017 dataset\n",
    "}\n",
    "seed_everything(args.seed) #固定随机数种子\n",
    "task_name = args.task_name.lower()\n",
    "#初始化输出的文件夹\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "args.output_dir = args.output_dir\n",
    "if os.path.exists(args.output_dir) and os.listdir(\n",
    "        args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir))\n",
    "#设置cuda\n",
    "if config.LOCAL_RANK == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(config.LOCAL_RANK)\n",
    "    device = torch.device(\"cuda\", config.LOCAL_RANK)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "processor = processors[task_name]()#获得读取tsv文件方法\n",
    "num_labels = num_labels_task[task_name]#判定几分类\n",
    "label_list = processor.get_labels()#获得分类标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bb07a-89a6-4764-831f-b0fd96519557",
   "metadata": {},
   "source": [
    "# 定义模型 载入预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d989f45-96e0-49fe-b181-039aeca4cb67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 16:56:38,460 - root - INFO - Loaded pretrained model file ./pretrains/pytorch_model_tiny.bin\n",
      "2023-11-21 16:56:38,972 - root - WARNING - Missing keys: [], unexpected_keys: ['deberta.embeddings.word_embeddings._weight', 'deberta.embeddings.position_embeddings._weight'], error_msgs: []\n",
      "2023-11-21 16:56:39,079 - root - INFO - Loaded pretrained model file ./pretrains/pytorch_model_tiny.bin\n",
      "2023-11-21 16:56:39,612 - root - WARNING - Missing keys: [], unexpected_keys: ['deberta.embeddings.word_embeddings._weight', 'deberta.embeddings.position_embeddings._weight'], error_msgs: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from ./pretrains/swin_tiny_patch4_window7_224.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.018)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.036)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.055)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.109)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.127)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.145)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.164)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.182)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.200)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = os.path.join(args.model_name_or_path, 'bert_config.json')\n",
    "bert_config = BertConfig.from_json_file(config_file)\n",
    "# 读取词表 方便之后把文字转数字id 返回的是一个30000词字典\n",
    "tokenizer = SPMTokenizer(args.vocab_path)\n",
    "#创建并导入预训练模型 这个模型用于文本encoder 融合图文feature\n",
    "model=SwinBERTa(args, bert_config,args.init_model)\n",
    "model.to(device)\n",
    "#设定图像encoder \n",
    "config = get_config(args)\n",
    "encoder = SwinTransformer(img_size=224,\n",
    "                          patch_size=config.MODEL.SWIN.PATCH_SIZE,\n",
    "                          in_chans=config.MODEL.SWIN.IN_CHANS,\n",
    "                          num_classes=config.MODEL.NUM_CLASSES,\n",
    "                          embed_dim=config.MODEL.SWIN.EMBED_DIM,\n",
    "                          depths=config.MODEL.SWIN.DEPTHS,\n",
    "                          num_heads=config.MODEL.SWIN.NUM_HEADS,\n",
    "                          window_size=config.MODEL.SWIN.WINDOW_SIZE,\n",
    "                          mlp_ratio=config.MODEL.SWIN.MLP_RATIO,\n",
    "                          qkv_bias=config.MODEL.SWIN.QKV_BIAS,\n",
    "                          qk_scale=config.MODEL.SWIN.QK_SCALE,\n",
    "                          drop_rate=config.MODEL.DROP_RATE,\n",
    "                          drop_path_rate=config.MODEL.DROP_PATH_RATE,\n",
    "                          patch_norm=config.MODEL.SWIN.PATCH_NORM,\n",
    "                          use_checkpoint=False)\n",
    "pretrained_dict = torch.load(args.img_ckpt, map_location='cpu')\n",
    "pretrained_dict = pretrained_dict['model']\n",
    "unexpected_keys = {\"head.weight\", \"head.bias\"}\n",
    "# 删除不匹配的键值\n",
    "for key in unexpected_keys:\n",
    "    del pretrained_dict[key]\n",
    "missing_keys, unexpected_keys = encoder.load_state_dict(pretrained_dict, strict=False)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d60ab-7d42-45bd-bf0f-4df840e90163",
   "metadata": {},
   "source": [
    "# 加载优化器 pth保存路径配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdb8aab-870e-4951-914b-77dd9117c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 16:56:41,877 - root - INFO - LOOKING AT ./twitterdataset/absa_data/twitter/train.tsv\n"
     ]
    }
   ],
   "source": [
    "train_examples = processor.get_train_examples(args.data_dir)#获取训练集文本内容\n",
    "eval_examples = processor.get_dev_examples(args.data_dir)#获取验证集文本内容\n",
    "num_train_steps = int(len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
    "t_total = num_train_steps\n",
    "#文本和融合部分参数的优化策略\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "#图像部分参数优化策略\n",
    "def check_keywords_in_name(name, keywords=()):\n",
    "    isin = False\n",
    "    for keyword in keywords:\n",
    "        if keyword in name:\n",
    "            isin = True\n",
    "    return isin\n",
    "def set_weight_decay(model, skip_list=(), skip_keywords=()):\n",
    "    has_decay = []\n",
    "    no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n",
    "                check_keywords_in_name(name, skip_keywords):\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            has_decay.append(param)\n",
    "    return [{'params': has_decay,'weight_decay': 0.01},\n",
    "            {'params': no_decay, 'weight_decay': 0.}]\n",
    "skip = {'absolute_pos_embed'}\n",
    "skip_keywords = {'relative_position_bias_table'}\n",
    "optimizer_grouped_parameters2 = set_weight_decay(encoder, skip, skip_keywords)\n",
    "#合并两组参数统一传入adamw优化器调参\n",
    "optimizer_grouped_parameters = optimizer_grouped_parameters1 + optimizer_grouped_parameters2\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate,eps=1e-6)\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "t_total = num_train_steps\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=int(t_total * args.warmup_proportion), t_total=t_total)\n",
    "output_model_file = os.path.join(args.output_dir, \"pytorch_model.pth\")\n",
    "output_encoder_file = os.path.join(args.output_dir, \"pytorch_encoder.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0741964-7b04-4bec-a153-b4506af1fde7",
   "metadata": {},
   "source": [
    "# 载入训练集 验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4570d1-b61a-4e0e-8c8b-ac2e3e86103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 16:56:41,928 - root - INFO - *** Example ***\n",
      "2023-11-21 16:56:41,929 - root - INFO - guid: train-1\n",
      "2023-11-21 16:56:41,929 - root - INFO - tokens: [CLS] ▁how ▁$ t $ ▁is ▁changing ▁the ▁influencer ▁game ▁: [SEP] ▁jake ▁paul [SEP]\n",
      "2023-11-21 16:56:41,930 - root - INFO - input_ids: 1 361 419 297 1814 269 2198 262 29655 522 877 2 109757 38723 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 16:56:41,930 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 16:56:41,931 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 16:56:41,931 - root - INFO - label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 134\n",
      "the max length of sentence a: 49 entity b: 10 total length: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 16:57:36,876 - root - INFO - *** Example ***\n",
      "2023-11-21 16:57:36,877 - root - INFO - guid: dev-1\n",
      "2023-11-21 16:57:36,878 - root - INFO - tokens: [CLS] ▁looking ▁forward ▁to ▁the ▁$ t $ ▁from ▁4 ▁- ▁8 ▁july ▁! ▁more ▁info ▁here ▁# ▁heritage ▁# ▁music [SEP] ▁f other ing hay ▁festival [SEP]\n",
      "2023-11-21 16:57:36,878 - root - INFO - input_ids: 1 562 939 264 262 419 297 1814 292 453 341 578 52434 1084 310 2470 422 953 5456 953 755 2 2994 10705 510 28577 3694 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 16:57:36,879 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 16:57:36,879 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 16:57:36,880 - root - INFO - label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 29\n",
      "the max length of sentence a: 48 entity b: 10 total length: 55\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_mm_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "            args.path_image)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in train_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in train_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in train_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in train_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                           all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids,\n",
    "                           all_img_feats, all_label_ids)\n",
    "if args.local_rank == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size,\n",
    "                              drop_last=True)\n",
    "# 获取验证集\n",
    "eval_features = convert_mm_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "    args.path_image)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in eval_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                          all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids, \\\n",
    "                          all_img_feats, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size, drop_last=True)\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "max_acc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2576d-968e-4f67-b827-48e0335ebc75",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a3d005a-22b4-48e1-bb7e-70278a137982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 16:57:55,506 - root - INFO - *************** Running training ***************\n",
      "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]2023-11-21 16:57:55,509 - root - INFO - ********** Epoch: 0 **********\n",
      "2023-11-21 16:57:55,510 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 16:57:55,510 - root - INFO -   Batch size = 24\n",
      "2023-11-21 16:57:55,511 - root - INFO -   Num steps = 1187\n",
      "Iteration:   0%|          | 0/148 [00:00<?, ?it/s]/usr/local/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Iteration (loss: 0.7891,lr:0.0000299417): 100%|██████████| 148/148 [00:25<00:00,  5.70it/s]\n",
      "2023-11-21 16:58:21,483 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 16:58:21,484 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 16:58:21,484 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.6312): 100%|██████████| 49/49 [00:03<00:00, 12.87it/s]\n",
      "2023-11-21 16:58:25,302 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 16:58:25,302 - root - INFO -   eval_accuracy = 0.5476190476190477\n",
      "2023-11-21 16:58:25,303 - root - INFO -   eval_loss = 0.9431161284446716\n",
      "2023-11-21 16:58:25,303 - root - INFO -   f_score = 0.464423209303255\n",
      "2023-11-21 16:58:25,304 - root - INFO -   global_step = 148\n",
      "2023-11-21 16:58:25,305 - root - INFO -   loss = 0.9950951313650286\n",
      "Epoch:  12%|█▎        | 1/8 [00:30<03:34, 30.69s/it]2023-11-21 16:58:26,195 - root - INFO - ********** Epoch: 1 **********\n",
      "2023-11-21 16:58:26,196 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 16:58:26,197 - root - INFO -   Batch size = 24\n",
      "2023-11-21 16:58:26,198 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.6514,lr:0.0000279941): 100%|██████████| 148/148 [00:25<00:00,  5.86it/s]\n",
      "2023-11-21 16:58:51,452 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 16:58:51,452 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 16:58:51,453 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.4834): 100%|██████████| 49/49 [00:03<00:00, 13.20it/s]\n",
      "2023-11-21 16:58:55,175 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 16:58:55,176 - root - INFO -   eval_accuracy = 0.6284013605442177\n",
      "2023-11-21 16:58:55,177 - root - INFO -   eval_loss = 0.8402965458071961\n",
      "2023-11-21 16:58:55,177 - root - INFO -   f_score = 0.6046890078688686\n",
      "2023-11-21 16:58:55,178 - root - INFO -   global_step = 296\n",
      "2023-11-21 16:58:55,178 - root - INFO -   loss = 0.7652462956470412\n",
      "Epoch:  25%|██▌       | 2/8 [01:00<03:01, 30.20s/it]2023-11-21 16:58:56,057 - root - INFO - ********** Epoch: 2 **********\n",
      "2023-11-21 16:58:56,058 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 16:58:56,059 - root - INFO -   Batch size = 24\n",
      "2023-11-21 16:58:56,059 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.4387,lr:0.0000236267): 100%|██████████| 148/148 [00:24<00:00,  5.94it/s]\n",
      "2023-11-21 16:59:20,992 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 16:59:20,992 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 16:59:20,993 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.6356): 100%|██████████| 49/49 [00:03<00:00, 13.10it/s]\n",
      "2023-11-21 16:59:24,744 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 16:59:24,744 - root - INFO -   eval_accuracy = 0.6164965986394558\n",
      "2023-11-21 16:59:24,745 - root - INFO -   eval_loss = 0.8630440046592635\n",
      "2023-11-21 16:59:24,745 - root - INFO -   f_score = 0.5608706690286897\n",
      "2023-11-21 16:59:24,746 - root - INFO -   global_step = 444\n",
      "2023-11-21 16:59:24,746 - root - INFO -   loss = 0.6072356799567068\n",
      "Epoch:  38%|███▊      | 3/8 [01:29<02:27, 29.51s/it]2023-11-21 16:59:24,747 - root - INFO - ********** Epoch: 3 **********\n",
      "2023-11-21 16:59:24,747 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 16:59:24,748 - root - INFO -   Batch size = 24\n",
      "2023-11-21 16:59:24,748 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.5148,lr:0.0000176529): 100%|██████████| 148/148 [00:25<00:00,  5.89it/s]\n",
      "2023-11-21 16:59:49,888 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 16:59:49,889 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 16:59:49,889 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.4810): 100%|██████████| 49/49 [00:03<00:00, 13.07it/s]\n",
      "2023-11-21 16:59:53,647 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 16:59:53,647 - root - INFO -   eval_accuracy = 0.6768707482993197\n",
      "2023-11-21 16:59:53,648 - root - INFO -   eval_loss = 0.8522942327723211\n",
      "2023-11-21 16:59:53,648 - root - INFO -   f_score = 0.6624188748422731\n",
      "2023-11-21 16:59:53,649 - root - INFO -   global_step = 592\n",
      "2023-11-21 16:59:53,649 - root - INFO -   loss = 0.4156085935396117\n",
      "Epoch:  50%|█████     | 4/8 [01:59<01:58, 29.64s/it]2023-11-21 16:59:54,580 - root - INFO - ********** Epoch: 4 **********\n",
      "2023-11-21 16:59:54,580 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 16:59:54,581 - root - INFO -   Batch size = 24\n",
      "2023-11-21 16:59:54,582 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.3884,lr:0.0000111852): 100%|██████████| 148/148 [00:25<00:00,  5.91it/s]\n",
      "2023-11-21 17:00:19,649 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 17:00:19,649 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 17:00:19,650 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.8383): 100%|██████████| 49/49 [00:03<00:00, 12.31it/s]\n",
      "2023-11-21 17:00:23,640 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 17:00:23,641 - root - INFO -   eval_accuracy = 0.6641156462585034\n",
      "2023-11-21 17:00:23,642 - root - INFO -   eval_loss = 1.0241642886278581\n",
      "2023-11-21 17:00:23,642 - root - INFO -   f_score = 0.6337809552191728\n",
      "2023-11-21 17:00:23,642 - root - INFO -   global_step = 740\n",
      "2023-11-21 17:00:23,643 - root - INFO -   loss = 0.2713249959555027\n",
      "Epoch:  62%|██████▎   | 5/8 [02:28<01:28, 29.43s/it]2023-11-21 17:00:23,644 - root - INFO - ********** Epoch: 5 **********\n",
      "2023-11-21 17:00:23,645 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 17:00:23,645 - root - INFO -   Batch size = 24\n",
      "2023-11-21 17:00:23,646 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.1210,lr:0.0000054278): 100%|██████████| 148/148 [00:24<00:00,  6.05it/s]\n",
      "2023-11-21 17:00:48,109 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 17:00:48,109 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 17:00:48,110 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.8521): 100%|██████████| 49/49 [00:03<00:00, 12.93it/s]\n",
      "2023-11-21 17:00:51,911 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 17:00:51,911 - root - INFO -   eval_accuracy = 0.673469387755102\n",
      "2023-11-21 17:00:51,912 - root - INFO -   eval_loss = 1.109388169585442\n",
      "2023-11-21 17:00:51,913 - root - INFO -   f_score = 0.6540798049793756\n",
      "2023-11-21 17:00:51,913 - root - INFO -   global_step = 888\n",
      "2023-11-21 17:00:51,914 - root - INFO -   loss = 0.16834384272177075\n",
      "Epoch:  75%|███████▌  | 6/8 [02:56<00:58, 29.04s/it]2023-11-21 17:00:51,915 - root - INFO - ********** Epoch: 6 **********\n",
      "2023-11-21 17:00:51,916 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 17:00:51,916 - root - INFO -   Batch size = 24\n",
      "2023-11-21 17:00:51,917 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.0461,lr:0.0000014529): 100%|██████████| 148/148 [00:25<00:00,  5.83it/s]\n",
      "2023-11-21 17:01:17,321 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 17:01:17,322 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 17:01:17,323 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.8804): 100%|██████████| 49/49 [00:03<00:00, 12.81it/s]\n",
      "2023-11-21 17:01:21,160 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 17:01:21,161 - root - INFO -   eval_accuracy = 0.6785714285714286\n",
      "2023-11-21 17:01:21,162 - root - INFO -   eval_loss = 1.1511015311187627\n",
      "2023-11-21 17:01:21,162 - root - INFO -   f_score = 0.6584743635314253\n",
      "2023-11-21 17:01:21,163 - root - INFO -   global_step = 1036\n",
      "2023-11-21 17:01:21,163 - root - INFO -   loss = 0.1280820703486333\n",
      "Epoch:  88%|████████▊ | 7/8 [03:26<00:29, 29.40s/it]2023-11-21 17:01:22,059 - root - INFO - ********** Epoch: 7 **********\n",
      "2023-11-21 17:01:22,060 - root - INFO -   Num examples = 3562\n",
      "2023-11-21 17:01:22,061 - root - INFO -   Batch size = 24\n",
      "2023-11-21 17:01:22,062 - root - INFO -   Num steps = 1187\n",
      "Iteration (loss: 0.1881,lr:0.0000000006): 100%|██████████| 148/148 [00:25<00:00,  5.88it/s]\n",
      "2023-11-21 17:01:47,257 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-21 17:01:47,258 - root - INFO -   Num examples = 1176\n",
      "2023-11-21 17:01:47,259 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.9044): 100%|██████████| 49/49 [00:03<00:00, 12.89it/s]\n",
      "2023-11-21 17:01:51,071 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-21 17:01:51,071 - root - INFO -   eval_accuracy = 0.6743197278911565\n",
      "2023-11-21 17:01:51,072 - root - INFO -   eval_loss = 1.1593707165851885\n",
      "2023-11-21 17:01:51,073 - root - INFO -   f_score = 0.6573660080869923\n",
      "2023-11-21 17:01:51,074 - root - INFO -   global_step = 1184\n",
      "2023-11-21 17:01:51,074 - root - INFO -   loss = 0.10513007538896557\n",
      "Epoch: 100%|██████████| 8/8 [03:55<00:00, 29.45s/it]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"*************** Running training ***************\")\n",
    "for train_idx in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "    logger.info(\"********** Epoch: \" + str(train_idx) + \" **********\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "    model.train()\n",
    "    encoder.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), desc=\"Iteration\", total=len(train_dataloader), position=0)\n",
    "    for step, batch in progress_bar:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "            img_feats, label_ids = batch\n",
    "        img_att = encoder(img_feats)\n",
    "        loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                         s2_input_mask, \\\n",
    "                         added_input_mask, label_ids)\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        scheduler.step()  # 使用学习率调整算法\n",
    "        for param_group in optimizer.param_groups:\n",
    "            progress_bar.set_description(f\"Iteration (loss: {loss.item():.4f},lr:{param_group['lr']:.10f})\")\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    logger.info(\"***** Running evaluation on Dev Set*****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    model.eval()\n",
    "    encoder.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_label_list = []\n",
    "    pred_label_list = []\n",
    "    #验证\n",
    "    progress_bar = tqdm(eval_dataloader, desc=\"Evaluating\", position=0)\n",
    "    for input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "            img_feats, label_ids in progress_bar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        added_input_mask = added_input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        s2_input_ids = s2_input_ids.to(device)\n",
    "        s2_input_mask = s2_input_mask.to(device)\n",
    "        s2_segment_ids = s2_segment_ids.to(device)\n",
    "        img_feats = img_feats.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            img_att = encoder(img_feats)\n",
    "            tmp_eval_loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids,\n",
    "                                  input_mask, s2_input_mask, added_input_mask, label_ids)\n",
    "            logits = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                           s2_input_mask, added_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        true_label_list.append(label_ids)\n",
    "        pred_label_list.append(logits)\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        progress_bar.set_description(f\"Evaluating (tmp_eval_loss: {tmp_eval_loss.item():.4f})\")\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    loss = tr_loss / nb_tr_steps if args.do_train else None\n",
    "    true_label = np.concatenate(true_label_list)\n",
    "    pred_outputs = np.concatenate(pred_label_list)\n",
    "    precision, recall, F_score = macro_f1(true_label, pred_outputs)\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "              'f_score': F_score,\n",
    "              'global_step': global_step,\n",
    "              'loss': loss}\n",
    "    logger.info(\"***** Dev Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "    if eval_accuracy >= max_acc:\n",
    "        # Save a trained model\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        encoder_to_save = encoder.module if hasattr(encoder,\n",
    "                                                    'module') else encoder  # Only save the model it-self\n",
    "        if args.do_train:\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "            torch.save(encoder_to_save.state_dict(), output_encoder_file)\n",
    "        max_acc = eval_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa03d8-dfde-4274-8d3f-b544be05ea97",
   "metadata": {},
   "source": [
    "# 取最好的结果在测试集上验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec52fd4-39df-45aa-a364-7d2c68529d65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 17:01:51,729 - root - INFO - *** Example ***\n",
      "2023-11-21 17:01:51,730 - root - INFO - guid: test-1\n",
      "2023-11-21 17:01:51,731 - root - INFO - tokens: [CLS] ▁# ▁$ t $ ▁performs ▁at ▁stagecoach ▁# ▁music festival ▁2016 [SEP] ▁sam hunt [SEP]\n",
      "2023-11-21 17:01:51,731 - root - INFO - input_ids: 1 953 419 297 1814 8993 288 109755 953 755 47550 892 2 20782 39396 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 17:01:51,732 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 17:01:51,733 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-21 17:01:51,734 - root - INFO - label: 2 (id = 2)\n",
      "2023-11-21 17:02:10,298 - root - INFO - ***** Running evaluation on Test Set*****\n",
      "2023-11-21 17:02:10,299 - root - INFO -   Num examples = 1234\n",
      "2023-11-21 17:02:10,299 - root - INFO -   Batch size = 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 62\n",
      "the max length of sentence a: 71 entity b: 10 total length: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 52/52 [00:04<00:00, 12.89it/s]\n",
      "2023-11-21 17:02:14,645 - root - INFO - ***** Test Eval results *****\n",
      "2023-11-21 17:02:14,646 - root - INFO -   eval_accuracy = 0.7009724473257699\n",
      "2023-11-21 17:02:14,646 - root - INFO -   eval_loss = 1.054577276110649\n",
      "2023-11-21 17:02:14,647 - root - INFO -   f_score = 0.6793481269484672\n",
      "2023-11-21 17:02:14,647 - root - INFO -   global_step = 1184\n",
      "2023-11-21 17:02:14,648 - root - INFO -   loss = 0.10513007538896557\n",
      "2023-11-21 17:02:14,648 - root - INFO -   precision = 0.6930598262933593\n",
      "2023-11-21 17:02:14,649 - root - INFO -   recall = 0.6689137410997378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.054577276110649, 'eval_accuracy': 0.7009724473257699, 'precision': 0.6930598262933593, 'recall': 0.6689137410997378, 'f_score': 0.6793481269484672, 'global_step': 1184, 'loss': 0.10513007538896557}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # 先把cuda清空了\n",
    "model.load_state_dict(torch.load(output_model_file))#载入eval最好的结果\n",
    "encoder.load_state_dict(torch.load(output_encoder_file))#载入eval最好的结果\n",
    "eval_examples = processor.get_test_examples(args.data_dir)#获得测试集\n",
    "eval_features = convert_mm_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "    args.path_image)\n",
    "logger.info(\"***** Running evaluation on Test Set*****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in eval_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                          all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids,\n",
    "                          all_img_feats, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "model.eval()\n",
    "encoder.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "true_label_list = []\n",
    "pred_label_list = []\n",
    "for input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "        img_feats, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    added_input_mask = added_input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    s2_input_ids = s2_input_ids.to(device)\n",
    "    s2_input_mask = s2_input_mask.to(device)\n",
    "    s2_segment_ids = s2_segment_ids.to(device)\n",
    "    img_feats = img_feats.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        img_att = encoder(img_feats)\n",
    "        tmp_eval_loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids,\n",
    "                              input_mask, s2_input_mask, added_input_mask, label_ids)\n",
    "        logits = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                       s2_input_mask, added_input_mask)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    true_label_list.append(label_ids)\n",
    "    pred_label_list.append(logits)\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "loss = tr_loss / nb_tr_steps if args.do_train else None\n",
    "true_label = np.concatenate(true_label_list)\n",
    "pred_outputs = np.concatenate(pred_label_list)\n",
    "precision, recall, F_score = macro_f1(true_label, pred_outputs)\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'precision': precision,\n",
    "          'recall': recall,\n",
    "          'f_score': F_score,\n",
    "          'global_step': global_step,\n",
    "          'loss': loss}\n",
    "pred_label = np.argmax(pred_outputs, axis=-1)\n",
    "fout_p = open(os.path.join(args.output_dir, \"pred.txt\"), 'w')\n",
    "fout_t = open(os.path.join(args.output_dir, \"true.txt\"), 'w')\n",
    "for i in range(len(pred_label)):\n",
    "    attstr = str(pred_label[i])\n",
    "    fout_p.write(attstr + '\\n')\n",
    "for i in range(len(true_label)):\n",
    "    attstr = str(true_label[i])\n",
    "    fout_t.write(attstr + '\\n')\n",
    "output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Test Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "fout_p.close()\n",
    "fout_t.close()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
