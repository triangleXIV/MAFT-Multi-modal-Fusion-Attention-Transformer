{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a53601-3667-489d-8763-ef88e06264c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: yacs in /usr/local/miniconda3/lib/python3.8/site-packages (0.1.8)\n",
      "Requirement already satisfied: timm in /usr/local/miniconda3/lib/python3.8/site-packages (0.9.10)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/miniconda3/lib/python3.8/site-packages (0.1.99)\n",
      "Requirement already satisfied: regex in /usr/local/miniconda3/lib/python3.8/site-packages (2023.10.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/miniconda3/lib/python3.8/site-packages (from yacs) (6.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.15.1+cu117)\n",
      "Requirement already satisfied: safetensors in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (0.19.4)\n",
      "Requirement already satisfied: torch>=1.7 in /usr/local/miniconda3/lib/python3.8/site-packages (from timm) (2.0.0+cu117)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (2.0.0)\n",
      "Requirement already satisfied: networkx in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (1.11.1)\n",
      "Requirement already satisfied: filelock in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/miniconda3/lib/python3.8/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: cmake in /usr/local/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7->timm) (15.0.7)\n",
      "Requirement already satisfied: requests in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2.29.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from huggingface-hub->timm) (2023.10.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from torchvision->timm) (9.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/miniconda3/lib/python3.8/site-packages (from torchvision->timm) (1.24.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/miniconda3/lib/python3.8/site-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/miniconda3/lib/python3.8/site-packages (from sympy->torch>=1.7->timm) (1.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yacs timm sentencepiece regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889c919a-b4e7-40c7-acc4-e14f59053eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from yacs.config import CfgNode as CN\n",
    "from tqdm import tqdm,trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from toolcls import AbmsaProcessor,seed_everything,convert_mm_examples_to_features,BertConfig\n",
    "from models.swin.swintransformer import SwinTransformer,get_config\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from models.deberta.spm_tokenizer import SPMTokenizer\n",
    "from models.deberta.deberta import SwinBERTa\n",
    "from models.logs import logger\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "# 无0.1\n",
    "try:\n",
    "    import safetensors.torch\n",
    "    _has_safetensors = True\n",
    "except ImportError:\n",
    "    _has_safetensors = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b10da-e9f2-4d6c-bffc-ae6e5b6fd01f",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cec1ab2-a125-4daa-8ee0-bb0c24fca243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# img_encoder config\n",
    "_C = CN()\n",
    "config = _C.clone()\n",
    "config.LOCAL_RANK = -1\n",
    "\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return (1-x)/(1-warmup)\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    preds = np.argmax(y_pred, axis=-1)\n",
    "    true = y_true\n",
    "    p_macro, r_macro, f_macro, support_macro \\\n",
    "      = precision_recall_fscore_support(true, preds, average='macro')\n",
    "    #f_macro = 2*p_macro*r_macro/(p_macro+r_macro)\n",
    "    return p_macro, r_macro, f_macro\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "## Required parameters\n",
    "parser.add_argument(\"--data_dir\",default='./twitterdataset/absa_data/twitter2015',type=str,#文本数据位置\n",
    "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "parser.add_argument(\"--img_ckpt\", default='./pretrains/swin_base_patch4_window7_224_1k.pth', type=str,#swin预训练模型的准确位置\n",
    "                    help=\"Bert pre-trained model selected in the list: S24-224,S24-336 \")\n",
    "parser.add_argument(\"--spm_model_file\",default='./pretrains/30k-clean.model',type=str)#albert预训练模型的准确位置\n",
    "parser.add_argument(\"--model_name_or_path\", default='./pretrains', type=str,#albert预训练模型存放的文件夹\n",
    "                    help=\"Path to pre-trained model or shortcut name selected in the list\")\n",
    "parser.add_argument(\"--task_name\",default='twitter15',type=str,#要加载哪个数据集 Twitter是17 Twitter15是15\n",
    "                    help=\"The name of the task to train.\")\n",
    "parser.add_argument(\"--output_dir\",default='./output',type=str,\n",
    "                    help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "parser.add_argument('--path_image', default='./twitterdataset/img_data/twitter2015_images',#图像的位置\n",
    "                    help='path to images')\n",
    "parser.add_argument('--init_model',\n",
    "                    type=str,\n",
    "                    default='./pretrains/pytorch_model.bin',\n",
    "                    help=\"The model state file used to initialize the model weights.\")\n",
    "parser.add_argument('--model_config',\n",
    "                    type=str,\n",
    "                    default='./pretrains/config.json',\n",
    "                    help=\"The config file of bert model.\")\n",
    "parser.add_argument('--pre_trained',\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    help=\"The path of pre-trained RoBERTa model\")\n",
    "parser.add_argument('--vocab_path',\n",
    "                    default='./pretrains/spm.model',\n",
    "                    type=str,\n",
    "                    help=\"The path of the vocabulary\")\n",
    "## Other parameters\n",
    "parser.add_argument('--crop_size', type=int, default=224, help='crop size of image')\n",
    "parser.add_argument(\"--max_seq_length\",default=64,type=int,\n",
    "                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--max_entity_length\",default=16,type=int,\n",
    "                    help=\"The maximum entity input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--do_train\",action='store_true',default=True,\n",
    "                    help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_lower_case\",action='store_true',default=True,\n",
    "                    help=\"Set this flag if you are using an uncased model.\")\n",
    "parser.add_argument(\"--train_batch_size\",default=24,type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\",default=24,type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--learning_rate\",default=3e-5,type=float,#可能会暴毙\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\",default=8.0,type=float,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\",default=0.1,type=float,\n",
    "                    help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                         \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",action='store_true',default=False,\n",
    "                    help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",type=int,default=-1,\n",
    "                    help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed',type=int,default=8,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument('--gradient_accumulation_steps',type=int,default=1,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument('--fp16',default=False,action='store_true',#kaggle没搞懂怎么导入apex 索性就32位吧\n",
    "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--loss_scale',type=float, default=0,\n",
    "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                         \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                         \"Positive power of 2: static loss scaling value.\\n\")\n",
    "parser.add_argument('--overwrite_output_dir', action='store_true',default=True,#是否覆盖原本的输出文件\n",
    "                    help=\"Overwrite the content of the output directory\")\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument('--cfg', type=str, default=\"./pretrains/swin_base_patch4_window7_224.yaml\", metavar=\"FILE\",\n",
    "                    help='path to config file', )\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e4b1e-0876-45c8-ba8d-f362f357e2f8",
   "metadata": {},
   "source": [
    "# 基础随机数 cuda 路径之类的东西配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b7ddc1-de3f-448d-8a66-3e80053920a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.task_name == \"twitter17\":\n",
    "    args.path_image = \"./twitterdataset/img_data/twitter2017_images\"\n",
    "elif args.task_name == \"twitter15\":\n",
    "    args.path_image = \"./twitterdataset/img_data/twitter2015_images\"\n",
    "else:\n",
    "    print(\"The task name is not right!\")\n",
    "processors = {\n",
    "        \"twitter15\": AbmsaProcessor,    # our twitter-2015 dataset\n",
    "        \"twitter17\": AbmsaProcessor         # our twitter-2017 dataset\n",
    "}\n",
    "num_labels_task = {\n",
    "    \"twitter15\": 3,                # our twitter-2015 dataset\n",
    "    \"twitter17\": 3                     # our twitter-2017 dataset\n",
    "}\n",
    "seed_everything(args.seed) #固定随机数种子\n",
    "task_name = args.task_name.lower()\n",
    "#初始化输出的文件夹\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.mkdir(args.output_dir)\n",
    "args.output_dir = args.output_dir\n",
    "if os.path.exists(args.output_dir) and os.listdir(\n",
    "        args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir))\n",
    "#设置cuda\n",
    "if config.LOCAL_RANK == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(config.LOCAL_RANK)\n",
    "    device = torch.device(\"cuda\", config.LOCAL_RANK)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "processor = processors[task_name]()#获得读取tsv文件方法\n",
    "num_labels = num_labels_task[task_name]#判定几分类\n",
    "label_list = processor.get_labels()#获得分类标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bb07a-89a6-4764-831f-b0fd96519557",
   "metadata": {},
   "source": [
    "# 定义模型 载入预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d989f45-96e0-49fe-b181-039aeca4cb67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 10:10:24,087 - root - INFO - Loaded pretrained model file ./pretrains/pytorch_model.bin\n",
      "2023-11-19 10:10:26,378 - root - WARNING - Missing keys: [], unexpected_keys: [], error_msgs: []\n",
      "2023-11-19 10:10:26,640 - root - INFO - Loaded pretrained model file ./pretrains/pytorch_model.bin\n",
      "2023-11-19 10:10:28,865 - root - WARNING - Missing keys: [], unexpected_keys: [], error_msgs: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from ./pretrains/swin_base_patch4_window7_224.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.022)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.043)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.065)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.109)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.130)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.152)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.174)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.196)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.217)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.239)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.261)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.283)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.304)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.326)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.348)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.370)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.391)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.413)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.435)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.457)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.478)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.500)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = os.path.join(args.model_name_or_path, 'bert_config.json')\n",
    "bert_config = BertConfig.from_json_file(config_file)\n",
    "# 读取词表 方便之后把文字转数字id 返回的是一个30000词字典\n",
    "tokenizer = SPMTokenizer(args.vocab_path)\n",
    "#创建并导入预训练模型 这个模型用于文本encoder 融合图文feature\n",
    "model=SwinBERTa(args, bert_config,args.init_model)\n",
    "model.to(device)\n",
    "#设定图像encoder \n",
    "config = get_config(args)\n",
    "encoder = SwinTransformer(img_size=224,\n",
    "                          patch_size=config.MODEL.SWIN.PATCH_SIZE,\n",
    "                          in_chans=config.MODEL.SWIN.IN_CHANS,\n",
    "                          num_classes=config.MODEL.NUM_CLASSES,\n",
    "                          embed_dim=config.MODEL.SWIN.EMBED_DIM,\n",
    "                          depths=config.MODEL.SWIN.DEPTHS,\n",
    "                          num_heads=config.MODEL.SWIN.NUM_HEADS,\n",
    "                          window_size=config.MODEL.SWIN.WINDOW_SIZE,\n",
    "                          mlp_ratio=config.MODEL.SWIN.MLP_RATIO,\n",
    "                          qkv_bias=config.MODEL.SWIN.QKV_BIAS,\n",
    "                          qk_scale=config.MODEL.SWIN.QK_SCALE,\n",
    "                          drop_rate=config.MODEL.DROP_RATE,\n",
    "                          drop_path_rate=config.MODEL.DROP_PATH_RATE,\n",
    "                          patch_norm=config.MODEL.SWIN.PATCH_NORM,\n",
    "                          use_checkpoint=False)\n",
    "pretrained_dict = torch.load(args.img_ckpt, map_location='cpu')\n",
    "pretrained_dict = pretrained_dict['model']\n",
    "unexpected_keys = {\"head.weight\", \"head.bias\"}\n",
    "# 删除不匹配的键值\n",
    "for key in unexpected_keys:\n",
    "    del pretrained_dict[key]\n",
    "missing_keys, unexpected_keys = encoder.load_state_dict(pretrained_dict, strict=False)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d60ab-7d42-45bd-bf0f-4df840e90163",
   "metadata": {},
   "source": [
    "# 加载优化器 pth保存路径配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdb8aab-870e-4951-914b-77dd9117c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 10:10:35,189 - root - INFO - LOOKING AT ./twitterdataset/absa_data/twitter2015/train.tsv\n"
     ]
    }
   ],
   "source": [
    "train_examples = processor.get_train_examples(args.data_dir)#获取训练集文本内容\n",
    "eval_examples = processor.get_dev_examples(args.data_dir)#获取验证集文本内容\n",
    "num_train_steps = int(len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
    "t_total = num_train_steps\n",
    "#文本和融合部分参数的优化策略\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "#图像部分参数优化策略\n",
    "def check_keywords_in_name(name, keywords=()):\n",
    "    isin = False\n",
    "    for keyword in keywords:\n",
    "        if keyword in name:\n",
    "            isin = True\n",
    "    return isin\n",
    "def set_weight_decay(model, skip_list=(), skip_keywords=()):\n",
    "    has_decay = []\n",
    "    no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n",
    "                check_keywords_in_name(name, skip_keywords):\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            has_decay.append(param)\n",
    "    return [{'params': has_decay,'weight_decay': 0.01},\n",
    "            {'params': no_decay, 'weight_decay': 0.}]\n",
    "skip = {'absolute_pos_embed'}\n",
    "skip_keywords = {'relative_position_bias_table'}\n",
    "optimizer_grouped_parameters2 = set_weight_decay(encoder, skip, skip_keywords)\n",
    "#合并两组参数统一传入adamw优化器调参\n",
    "optimizer_grouped_parameters = optimizer_grouped_parameters1 + optimizer_grouped_parameters2\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate,eps=1e-6)\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "t_total = num_train_steps\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=int(t_total * args.warmup_proportion), t_total=t_total)\n",
    "output_model_file = os.path.join(args.output_dir, \"pytorch_model.pth\")\n",
    "output_encoder_file = os.path.join(args.output_dir, \"pytorch_encoder.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0741964-7b04-4bec-a153-b4506af1fde7",
   "metadata": {},
   "source": [
    "# 载入训练集 验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4570d1-b61a-4e0e-8c8b-ac2e3e86103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 10:10:35,278 - root - INFO - *** Example ***\n",
      "2023-11-19 10:10:35,279 - root - INFO - guid: train-1\n",
      "2023-11-19 10:10:35,280 - root - INFO - tokens: [CLS] ▁r t ▁@ ▁l t s chuck bass ▁: ▁$ t $ ▁is ▁everything ▁# ▁mc m [SEP] ▁chuck ▁bass [SEP]\n",
      "2023-11-19 10:10:35,282 - root - INFO - input_ids: 1 3638 297 1944 2531 297 268 46790 28320 877 419 297 1814 269 758 953 29894 358 2 28521 5839 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-19 10:10:35,283 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-19 10:10:35,284 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-19 10:10:35,285 - root - INFO - label: 2 (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 82\n",
      "the max length of sentence a: 51 entity b: 12 total length: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 10:12:32,401 - root - INFO - *** Example ***\n",
      "2023-11-19 10:12:32,404 - root - INFO - guid: dev-1\n",
      "2023-11-19 10:12:32,405 - root - INFO - tokens: [CLS] ▁r t ▁@ ▁funds over bun s ▁: ▁$ t $ ▁went ▁from ▁ped ophile ▁to ▁messing ▁with ▁cougar s ▁all ▁within ▁a ▁week [SEP] ▁ty ga [SEP]\n",
      "2023-11-19 10:12:32,407 - root - INFO - input_ids: 1 3638 297 1944 2145 4600 37238 268 877 419 297 1814 700 292 44070 59710 264 25066 275 48225 268 305 546 266 542 2 32976 6752 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-19 10:12:32,407 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-19 10:12:32,408 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-19 10:12:32,409 - root - INFO - label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 30\n",
      "the max length of sentence a: 54 entity b: 11 total length: 60\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_mm_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "            args.path_image)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in train_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in train_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in train_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in train_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                           all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids,\n",
    "                           all_img_feats, all_label_ids)\n",
    "if args.local_rank == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size,\n",
    "                              drop_last=True)\n",
    "# 获取验证集\n",
    "eval_features = convert_mm_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "    args.path_image)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in eval_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                          all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids, \\\n",
    "                          all_img_feats, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size, drop_last=True)\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "max_acc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2576d-968e-4f67-b827-48e0335ebc75",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a3d005a-22b4-48e1-bb7e-70278a137982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 10:13:10,604 - root - INFO - *************** Running training ***************\n",
      "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]2023-11-19 10:13:10,610 - root - INFO - ********** Epoch: 0 **********\n",
      "2023-11-19 10:13:10,611 - root - INFO -   Num examples = 3179\n",
      "2023-11-19 10:13:10,612 - root - INFO -   Batch size = 24\n",
      "2023-11-19 10:13:10,613 - root - INFO -   Num steps = 1059\n",
      "Iteration:   0%|          | 0/132 [00:00<?, ?it/s]/usr/local/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Iteration (loss: 0.4893,lr:0.0000299407): 100%|██████████| 132/132 [01:04<00:00,  2.06it/s]\n",
      "2023-11-19 10:14:14,834 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-19 10:14:14,835 - root - INFO -   Num examples = 1122\n",
      "2023-11-19 10:14:14,836 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.5161): 100%|██████████| 46/46 [00:09<00:00,  4.82it/s]\n",
      "2023-11-19 10:14:24,412 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-19 10:14:24,413 - root - INFO -   eval_accuracy = 0.6403985507246377\n",
      "2023-11-19 10:14:24,415 - root - INFO -   eval_loss = 0.7964397131100945\n",
      "2023-11-19 10:14:24,416 - root - INFO -   f_score = 0.5320932801164746\n",
      "2023-11-19 10:14:24,417 - root - INFO -   global_step = 132\n",
      "2023-11-19 10:14:24,419 - root - INFO -   loss = 0.9500181115034855\n",
      "Epoch:  12%|█▎        | 1/8 [01:17<09:05, 78.00s/it]2023-11-19 10:14:28,613 - root - INFO - ********** Epoch: 1 **********\n",
      "2023-11-19 10:14:28,614 - root - INFO -   Num examples = 3179\n",
      "2023-11-19 10:14:28,615 - root - INFO -   Batch size = 24\n",
      "2023-11-19 10:14:28,616 - root - INFO -   Num steps = 1059\n",
      "Iteration (loss: 0.7954,lr:0.0000279904): 100%|██████████| 132/132 [01:03<00:00,  2.08it/s]\n",
      "2023-11-19 10:15:31,941 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-19 10:15:31,943 - root - INFO -   Num examples = 1122\n",
      "2023-11-19 10:15:31,944 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.4265): 100%|██████████| 46/46 [00:09<00:00,  5.00it/s]\n",
      "2023-11-19 10:15:41,165 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-19 10:15:41,166 - root - INFO -   eval_accuracy = 0.6893115942028986\n",
      "2023-11-19 10:15:41,167 - root - INFO -   eval_loss = 0.7557735190443371\n",
      "2023-11-19 10:15:41,168 - root - INFO -   f_score = 0.5299168952030481\n",
      "2023-11-19 10:15:41,169 - root - INFO -   global_step = 264\n",
      "2023-11-19 10:15:41,170 - root - INFO -   loss = 0.6551375759370399\n",
      "Epoch:  25%|██▌       | 2/8 [02:34<07:43, 77.22s/it]2023-11-19 10:15:45,280 - root - INFO - ********** Epoch: 2 **********\n",
      "2023-11-19 10:15:45,282 - root - INFO -   Num examples = 3179\n",
      "2023-11-19 10:15:45,283 - root - INFO -   Batch size = 24\n",
      "2023-11-19 10:15:45,285 - root - INFO -   Num steps = 1059\n",
      "Iteration (loss: 0.3683,lr:0.0000236239): 100%|██████████| 132/132 [01:03<00:00,  2.09it/s]\n",
      "2023-11-19 10:16:48,323 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-19 10:16:48,324 - root - INFO -   Num examples = 1122\n",
      "2023-11-19 10:16:48,325 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.4763): 100%|██████████| 46/46 [00:10<00:00,  4.57it/s]\n",
      "2023-11-19 10:16:58,412 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-19 10:16:58,414 - root - INFO -   eval_accuracy = 0.7364130434782609\n",
      "2023-11-19 10:16:58,415 - root - INFO -   eval_loss = 0.7234622528371604\n",
      "2023-11-19 10:16:58,415 - root - INFO -   f_score = 0.677937414674079\n",
      "2023-11-19 10:16:58,416 - root - INFO -   global_step = 396\n",
      "2023-11-19 10:16:58,417 - root - INFO -   loss = 0.41695928980003705\n",
      "Epoch:  38%|███▊      | 3/8 [03:51<06:26, 77.25s/it]2023-11-19 10:17:02,575 - root - INFO - ********** Epoch: 3 **********\n",
      "2023-11-19 10:17:02,577 - root - INFO -   Num examples = 3179\n",
      "2023-11-19 10:17:02,579 - root - INFO -   Batch size = 24\n",
      "2023-11-19 10:17:02,580 - root - INFO -   Num steps = 1059\n",
      "Iteration (loss: 0.1965,lr:0.0000176534): 100%|██████████| 132/132 [01:00<00:00,  2.19it/s]\n",
      "2023-11-19 10:18:02,885 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-19 10:18:02,886 - root - INFO -   Num examples = 1122\n",
      "2023-11-19 10:18:02,887 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.6972): 100%|██████████| 46/46 [00:09<00:00,  4.98it/s]\n",
      "2023-11-19 10:18:12,154 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-19 10:18:12,154 - root - INFO -   eval_accuracy = 0.697463768115942\n",
      "2023-11-19 10:18:12,155 - root - INFO -   eval_loss = 0.9211091622710228\n",
      "2023-11-19 10:18:12,157 - root - INFO -   f_score = 0.6645151156061048\n",
      "2023-11-19 10:18:12,157 - root - INFO -   global_step = 528\n",
      "2023-11-19 10:18:12,158 - root - INFO -   loss = 0.24393656451932408\n",
      "Epoch:  50%|█████     | 4/8 [05:01<04:56, 74.23s/it]2023-11-19 10:18:12,161 - root - INFO - ********** Epoch: 4 **********\n",
      "2023-11-19 10:18:12,161 - root - INFO -   Num examples = 3179\n",
      "2023-11-19 10:18:12,163 - root - INFO -   Batch size = 24\n",
      "2023-11-19 10:18:12,164 - root - INFO -   Num steps = 1059\n",
      "Iteration (loss: 0.0459,lr:0.0000111893): 100%|██████████| 132/132 [01:00<00:00,  2.16it/s]\n",
      "2023-11-19 10:19:13,172 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-19 10:19:13,173 - root - INFO -   Num examples = 1122\n",
      "2023-11-19 10:19:13,174 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 0.9043): 100%|██████████| 46/46 [00:09<00:00,  4.90it/s]\n",
      "2023-11-19 10:19:22,590 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-19 10:19:22,591 - root - INFO -   eval_accuracy = 0.7463768115942029\n",
      "2023-11-19 10:19:22,592 - root - INFO -   eval_loss = 1.0018174298431561\n",
      "2023-11-19 10:19:22,593 - root - INFO -   f_score = 0.6689101633555875\n",
      "2023-11-19 10:19:22,594 - root - INFO -   global_step = 660\n",
      "2023-11-19 10:19:22,595 - root - INFO -   loss = 0.1406706343546058\n",
      "Epoch:  62%|██████▎   | 5/8 [06:16<03:42, 74.31s/it]2023-11-19 10:19:26,636 - root - INFO - ********** Epoch: 5 **********\n",
      "2023-11-19 10:19:26,638 - root - INFO -   Num examples = 3179\n",
      "2023-11-19 10:19:26,640 - root - INFO -   Batch size = 24\n",
      "2023-11-19 10:19:26,641 - root - INFO -   Num steps = 1059\n",
      "Iteration (loss: 0.0597,lr:0.0000054341): 100%|██████████| 132/132 [01:08<00:00,  1.94it/s]\n",
      "2023-11-19 10:20:34,772 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-19 10:20:34,773 - root - INFO -   Num examples = 1122\n",
      "2023-11-19 10:20:34,774 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 1.1440): 100%|██████████| 46/46 [00:09<00:00,  4.82it/s]\n",
      "2023-11-19 10:20:44,338 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-19 10:20:44,338 - root - INFO -   eval_accuracy = 0.7472826086956522\n",
      "2023-11-19 10:20:44,339 - root - INFO -   eval_loss = 1.0896331871981206\n",
      "2023-11-19 10:20:44,340 - root - INFO -   f_score = 0.7039453669833416\n",
      "2023-11-19 10:20:44,341 - root - INFO -   global_step = 792\n",
      "2023-11-19 10:20:44,342 - root - INFO -   loss = 0.04731395320303625\n",
      "Epoch:  75%|███████▌  | 6/8 [07:37<02:33, 76.85s/it]2023-11-19 10:20:48,411 - root - INFO - ********** Epoch: 6 **********\n",
      "2023-11-19 10:20:48,413 - root - INFO -   Num examples = 3179\n",
      "2023-11-19 10:20:48,414 - root - INFO -   Batch size = 24\n",
      "2023-11-19 10:20:48,415 - root - INFO -   Num steps = 1059\n",
      "Iteration (loss: 0.0022,lr:0.0000014580): 100%|██████████| 132/132 [01:00<00:00,  2.19it/s]\n",
      "2023-11-19 10:21:48,673 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-19 10:21:48,674 - root - INFO -   Num examples = 1122\n",
      "2023-11-19 10:21:48,675 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 1.1403): 100%|██████████| 46/46 [00:09<00:00,  4.74it/s]\n",
      "2023-11-19 10:21:58,401 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-19 10:21:58,402 - root - INFO -   eval_accuracy = 0.7581521739130435\n",
      "2023-11-19 10:21:58,403 - root - INFO -   eval_loss = 1.1167326980958814\n",
      "2023-11-19 10:21:58,404 - root - INFO -   f_score = 0.6979507452437779\n",
      "2023-11-19 10:21:58,405 - root - INFO -   global_step = 924\n",
      "2023-11-19 10:21:58,406 - root - INFO -   loss = 0.023502689283552834\n",
      "Epoch:  88%|████████▊ | 7/8 [08:51<01:15, 75.95s/it]2023-11-19 10:22:02,517 - root - INFO - ********** Epoch: 7 **********\n",
      "2023-11-19 10:22:02,519 - root - INFO -   Num examples = 3179\n",
      "2023-11-19 10:22:02,521 - root - INFO -   Batch size = 24\n",
      "2023-11-19 10:22:02,522 - root - INFO -   Num steps = 1059\n",
      "Iteration (loss: 0.0325,lr:0.0000000007): 100%|██████████| 132/132 [01:00<00:00,  2.18it/s]\n",
      "2023-11-19 10:23:03,070 - root - INFO - ***** Running evaluation on Dev Set*****\n",
      "2023-11-19 10:23:03,072 - root - INFO -   Num examples = 1122\n",
      "2023-11-19 10:23:03,073 - root - INFO -   Batch size = 24\n",
      "Evaluating (tmp_eval_loss: 1.1496): 100%|██████████| 46/46 [00:09<00:00,  4.81it/s]\n",
      "2023-11-19 10:23:12,652 - root - INFO - ***** Dev Eval results *****\n",
      "2023-11-19 10:23:12,653 - root - INFO -   eval_accuracy = 0.7572463768115942\n",
      "2023-11-19 10:23:12,654 - root - INFO -   eval_loss = 1.117178845340791\n",
      "2023-11-19 10:23:12,655 - root - INFO -   f_score = 0.7018904037031884\n",
      "2023-11-19 10:23:12,656 - root - INFO -   global_step = 1056\n",
      "2023-11-19 10:23:12,656 - root - INFO -   loss = 0.01873797818640896\n",
      "Epoch: 100%|██████████| 8/8 [10:02<00:00, 75.26s/it]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"*************** Running training ***************\")\n",
    "for train_idx in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "    logger.info(\"********** Epoch: \" + str(train_idx) + \" **********\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "    model.train()\n",
    "    encoder.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), desc=\"Iteration\", total=len(train_dataloader), position=0)\n",
    "    for step, batch in progress_bar:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "            img_feats, label_ids = batch\n",
    "        img_att = encoder(img_feats)\n",
    "        loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                         s2_input_mask, \\\n",
    "                         added_input_mask, label_ids)\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        scheduler.step()  # 使用学习率调整算法\n",
    "        for param_group in optimizer.param_groups:\n",
    "            progress_bar.set_description(f\"Iteration (loss: {loss.item():.4f},lr:{param_group['lr']:.10f})\")\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    logger.info(\"***** Running evaluation on Dev Set*****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    model.eval()\n",
    "    encoder.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_label_list = []\n",
    "    pred_label_list = []\n",
    "    #验证\n",
    "    progress_bar = tqdm(eval_dataloader, desc=\"Evaluating\", position=0)\n",
    "    for input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "            img_feats, label_ids in progress_bar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        added_input_mask = added_input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        s2_input_ids = s2_input_ids.to(device)\n",
    "        s2_input_mask = s2_input_mask.to(device)\n",
    "        s2_segment_ids = s2_segment_ids.to(device)\n",
    "        img_feats = img_feats.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            img_att = encoder(img_feats)\n",
    "            tmp_eval_loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids,\n",
    "                                  input_mask, s2_input_mask, added_input_mask, label_ids)\n",
    "            logits = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                           s2_input_mask, added_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        true_label_list.append(label_ids)\n",
    "        pred_label_list.append(logits)\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        progress_bar.set_description(f\"Evaluating (tmp_eval_loss: {tmp_eval_loss.item():.4f})\")\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    loss = tr_loss / nb_tr_steps if args.do_train else None\n",
    "    true_label = np.concatenate(true_label_list)\n",
    "    pred_outputs = np.concatenate(pred_label_list)\n",
    "    precision, recall, F_score = macro_f1(true_label, pred_outputs)\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "              'f_score': F_score,\n",
    "              'global_step': global_step,\n",
    "              'loss': loss}\n",
    "    logger.info(\"***** Dev Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "    if eval_accuracy >= max_acc:\n",
    "        # Save a trained model\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        encoder_to_save = encoder.module if hasattr(encoder,\n",
    "                                                    'module') else encoder  # Only save the model it-self\n",
    "        if args.do_train:\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "            torch.save(encoder_to_save.state_dict(), output_encoder_file)\n",
    "        max_acc = eval_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa03d8-dfde-4274-8d3f-b544be05ea97",
   "metadata": {},
   "source": [
    "# 取最好的结果在测试集上验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec52fd4-39df-45aa-a364-7d2c68529d65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 10:23:14,288 - root - INFO - *** Example ***\n",
      "2023-11-19 10:23:14,291 - root - INFO - guid: test-1\n",
      "2023-11-19 10:23:14,292 - root - INFO - tokens: [CLS] ▁r t ▁@ ▁ou ▁football ▁: ▁practice ▁one ▁in ▁the ▁books ▁want ▁more ▁photos ▁? ▁follow ▁the ▁official ▁$ t $ ▁football ▁facebook ▁page ▁! ▁# ▁only one [SEP] ▁oklahoma [SEP]\n",
      "2023-11-19 10:23:14,293 - root - INFO - input_ids: 1 3638 297 1944 17408 2242 877 1105 311 267 262 1116 409 310 1485 1102 1111 262 1658 419 297 1814 2242 9684 664 1084 953 364 2268 2 89637 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-19 10:23:14,294 - root - INFO - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-19 10:23:14,295 - root - INFO - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2023-11-19 10:23:14,296 - root - INFO - label: 1 (id = 1)\n",
      "2023-11-19 10:23:33,182 - root - INFO - ***** Running evaluation on Test Set*****\n",
      "2023-11-19 10:23:33,184 - root - INFO -   Num examples = 1037\n",
      "2023-11-19 10:23:33,186 - root - INFO -   Batch size = 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of problematic samples: 27\n",
      "the max length of sentence a: 44 entity b: 10 total length: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 44/44 [00:09<00:00,  4.73it/s]\n",
      "2023-11-19 10:23:44,203 - root - INFO - ***** Test Eval results *****\n",
      "2023-11-19 10:23:44,205 - root - INFO -   eval_accuracy = 0.7839922854387656\n",
      "2023-11-19 10:23:44,205 - root - INFO -   eval_loss = 0.944813069125468\n",
      "2023-11-19 10:23:44,207 - root - INFO -   f_score = 0.7291275306717931\n",
      "2023-11-19 10:23:44,208 - root - INFO -   global_step = 1056\n",
      "2023-11-19 10:23:44,209 - root - INFO -   loss = 0.01873797818640896\n",
      "2023-11-19 10:23:44,210 - root - INFO -   precision = 0.7543738153915346\n",
      "2023-11-19 10:23:44,211 - root - INFO -   recall = 0.7102345221582799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.944813069125468, 'eval_accuracy': 0.7839922854387656, 'precision': 0.7543738153915346, 'recall': 0.7102345221582799, 'f_score': 0.7291275306717931, 'global_step': 1056, 'loss': 0.01873797818640896}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # 先把cuda清空了\n",
    "model.load_state_dict(torch.load(output_model_file))#载入eval最好的结果\n",
    "encoder.load_state_dict(torch.load(output_encoder_file))#载入eval最好的结果\n",
    "eval_examples = processor.get_test_examples(args.data_dir)#获得测试集\n",
    "eval_features = convert_mm_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, args.max_entity_length, tokenizer, args.crop_size,\n",
    "    args.path_image)\n",
    "logger.info(\"***** Running evaluation on Test Set*****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_added_input_mask = torch.tensor([f.added_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_ids = torch.tensor([f.s2_input_ids for f in eval_features], dtype=torch.long)\n",
    "all_s2_input_mask = torch.tensor([f.s2_input_mask for f in eval_features], dtype=torch.long)\n",
    "all_s2_segment_ids = torch.tensor([f.s2_segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_img_feats = torch.stack([f.img_feat for f in eval_features])\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_added_input_mask, all_segment_ids, \\\n",
    "                          all_s2_input_ids, all_s2_input_mask, all_s2_segment_ids,\n",
    "                          all_img_feats, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "model.eval()\n",
    "encoder.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "true_label_list = []\n",
    "pred_label_list = []\n",
    "for input_ids, input_mask, added_input_mask, segment_ids, s2_input_ids, s2_input_mask, s2_segment_ids, \\\n",
    "        img_feats, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    added_input_mask = added_input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    s2_input_ids = s2_input_ids.to(device)\n",
    "    s2_input_mask = s2_input_mask.to(device)\n",
    "    s2_segment_ids = s2_segment_ids.to(device)\n",
    "    img_feats = img_feats.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        img_att = encoder(img_feats)\n",
    "        tmp_eval_loss = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids,\n",
    "                              input_mask, s2_input_mask, added_input_mask, label_ids)\n",
    "        logits = model(input_ids, s2_input_ids, img_att, segment_ids, s2_segment_ids, input_mask,\n",
    "                       s2_input_mask, added_input_mask)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    true_label_list.append(label_ids)\n",
    "    pred_label_list.append(logits)\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "loss = tr_loss / nb_tr_steps if args.do_train else None\n",
    "true_label = np.concatenate(true_label_list)\n",
    "pred_outputs = np.concatenate(pred_label_list)\n",
    "precision, recall, F_score = macro_f1(true_label, pred_outputs)\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'precision': precision,\n",
    "          'recall': recall,\n",
    "          'f_score': F_score,\n",
    "          'global_step': global_step,\n",
    "          'loss': loss}\n",
    "pred_label = np.argmax(pred_outputs, axis=-1)\n",
    "fout_p = open(os.path.join(args.output_dir, \"pred.txt\"), 'w')\n",
    "fout_t = open(os.path.join(args.output_dir, \"true.txt\"), 'w')\n",
    "for i in range(len(pred_label)):\n",
    "    attstr = str(pred_label[i])\n",
    "    fout_p.write(attstr + '\\n')\n",
    "for i in range(len(true_label)):\n",
    "    attstr = str(true_label[i])\n",
    "    fout_t.write(attstr + '\\n')\n",
    "output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Test Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "fout_p.close()\n",
    "fout_t.close()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
